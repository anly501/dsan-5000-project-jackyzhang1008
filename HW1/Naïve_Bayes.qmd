---
title: "Naïve Bayes"
---

# **Introduction to Naive Bayes**

## Overview

Naive Bayes is a classification technique based on applying Bayes’ theorem with a strong (naive) assumption that all the features are independent of each other. Despite its simplicity, Naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering.

## How it Works

The model is trained on the training dataset using Bayes’ theorem, and the maximum likelihood method is used to estimate the parameters. To classify a new instance, the likelihood of that instance is calculated for each class, and the class with the highest likelihood is assigned to the instance.

## Bayes’ Theorem

$$
P(C \mid X)=\frac{P(X \mid C) \cdot P(C)}{P(X)}
$$

As,

-   $P(C \mid X)$ is the posterior probability of class $C$ given predictor $X$.
-   $P(C)$ is the prior probability of class.
-   $P(X \mid C)$ is the likelihood which is the probability of predictor given class.
-   $P(X)$ is the prior probability of predictor.

## Objectives

The main objective of using Naive Bayes classification is to classify based on utilization of power plant in the future based on the observed characteristics, in this case will be the mean from 2013-2021; assuming the features are independent.

## Aim to Achieve

1.  Predict the 2022 utilization for each reactor based on the mean from 2013-2021, anything that is over 90 will satisfy as High utilization and below 90 will be Low utilization.
2.  Analysis text data to determine if they are related to the key word: "nuclear", "radiation", "reactor", "uranium", and "plutonium"

## Variants of Naive Bayes

1.  **Gaussian Naive Bayes**: Assumes that the features follow a normal distribution. This variant is used in cases where continuous values are associated with each class.

2.  **Multinomial Naive Bayes**: It is used for discrete counts. For example, let’s say we have a text classification problem. Here we can consider the occurrence of words as an event. In such a case, we will use a Multinomial Naive Bayes classifier.

3.  **Bernoulli Naive Bayes**: This is similar to the Multinomial Naive Bayes but the predictors are boolean variables. So, the parameters that are learned are the probabilities of the different categories and the probabilities of the predictors in each category. This variant is used when the feature vectors are binary (i.e., zeros and ones).

# **Record Data**

## **Prepare for Naïve Bayes**

```{python}
# handling the outliers
# Set any utilization factor in colum 2013-2022 above 100 to 100 in df
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)


df = pd.read_csv('../data/Cleaned/EDA/Nuclear_Energy_Utilization_Factor.csv')
print(df.describe())
df[df.columns[1:]] = df[df.columns[1:]].apply(lambda x: np.where(x > 100, 100, x))
print(df.describe())
```

This part is just like data cleaning, I am aiming to make sure that the utilization fill into the range from 0 to 100%. But in the real world there are cases where utilization goes over 100%, meaning that there could be more places that need more electrocites that the power plane can generate. For this case I am just going to make anything that is over 100% to 100%.

## **Feature Selection**

```{python}
# Calculate the mean utilization factor from 2013 to 2021
df['Mean_Utilization_2013_2021'] = df.loc[:, '2013':'2021'].mean(axis=1)

# Convert the utilization factor for 2022 into categories
df['Utilization_2022_Category'] = df['2022'].apply(lambda x: 'High' if x >= 90 else 'Low')

# Prepare the data
X = df[['Mean_Utilization_2013_2021']]
y = df['Utilization_2022_Category']

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

This part prepares data for the Naive Bayes Classifier, as all the columns are concerning each other as they are just utilization reports of each year. Our goal for the Native Bayes Classifier is to find out the utilization rate of future years from that date of the year before.

## **Naïve Bayes**

```{python}
# Initialize the Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Train the classifier
gnb.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = gnb.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

### **Accuracy and General Performance**

-   The model achieved an accuracy of 80%, which indicates a relatively high level of accuracy in predictions. However, this is based on a small test set of 10 instances, which might not be sufficient to generalize the model’s performance to unseen data.

### **Precision and Recall**

-   The model showed better precision and recall for the 'High' category compared to the 'Low' category. This indicates that the model is more reliable when predicting high utilization factors.

    -   **High Utilization Category**: Both precision and recall are 0.86, showing a balanced performance.

    -   **Low Utilization Category**: Both precision and recall are 0.67, which is lower compared to the 'High' category. This could be a point of improvement for the model.

### **F1-Score**

-   The F1-Score is also higher for the 'High' category (0.86) compared to the 'Low' category (0.67). Since F1-Score is the harmonic mean of precision and recall, this again highlights the model's stronger performance in predicting high utilization factors.

### **Confusion Matrix**

-   The confusion matrix reveals that the model made very few mistakes, with only one false positive and one false negative. However, due to the small size of the test set, each mistake has a significant impact on the performance metrics.

### **Conclusion**

-   The Gaussian Naive Bayes classifier provides a good starting point for predicting nuclear energy utilization categories based on historical data. The model demonstrates decent accuracy, but there is potential for enhancement, particularly in improving precision and recall for the "Low" category. The exploratory data analysis and visualizations offer valuable insights into the distribution and trends of utilization factors across different power plants. Future work could explore more sophisticated models, feature engineering, or additional data sources to further refine the predictions and gain deeper insights.​

# **Text Data**

## **Prepare for Naïve Bayes**

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

with open('../data/Cleaned/Energy_Policy.txt', 'r', encoding='utf-8') as file:
    text_data = file.read()
```

This part is just input the text data that are been extracted from pdf in the cleaning tab.

## **Feature Selection**

```{python}
# Define keywords related to nuclear energy
nuclear_keywords = ["nuclear", "radiation", "reactor", "uranium", "plutonium"]

# Segment the text into sentences
sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text_data)

# Function to label sentences based on the presence of nuclear-related keywords
def label_sentences(sentences, keywords):
    labels = []
    for sentence in sentences:
        if any(keyword.lower() in sentence.lower() for keyword in keywords):
            labels.append(1)  # Positive class (related to nuclear energy)
        else:
            labels.append(0)  # Negative class (not related to nuclear energy)
    return labels

# Label the sentences
labels = label_sentences(sentences, nuclear_keywords)


# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stop words and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Function to preprocess text
def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = text.split()
    # Lowercase and remove stop words
    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]
    return ' '.join(tokens)

# Preprocess each sentence
preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]

# Splitting the data into training, validation, and testing sets (80-10-10 split)
X_train, X_temp, y_train, y_temp = train_test_split(preprocessed_sentences, labels, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Initialize a TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Transform the text data into TF-IDF vectors
X_train_tfidf = vectorizer.fit_transform(X_train)
X_val_tfidf = vectorizer.transform(X_val)
X_test_tfidf = vectorizer.transform(X_test)
```

### **Step 1: Labeling the Data**

1.  **Define Keywords**: We'll use the keywords "nuclear", "radiation", "reactor", "uranium", and "plutonium" to identify text related to nuclear energy.

2.  **Segment the Text**: Divide the text into sentences.

3.  **Label the Segments**: Label each sentence based on whether it contains any of the defined keywords. Sentences with keywords will be labeled as 1 (positive class), and sentences without keywords will be labeled as 0 (negative class).

### **Step 2: Preprocessing the Data**

Next, we will pre process the data. This involves the following steps:

1.  **Cleaning the Text**: Removing any special characters, numbers, or unnecessary white space.

2.  **Tokenization**: Splitting the text into individual words.

3.  **Lowercasing**: Converting all text to lowercase to ensure uniformity.

4.  **Removing Stop Words**: Removing common words that do not contribute much meaning to the text.

5.  **Stemming or Lemmatization**: Reducing words to their base or root form.

## **Naïve Bayes**

```{python}
# Initialize a Multinomial Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the Naive Bayes classifier
nb_classifier.fit(X_train_tfidf, y_train)

# Make predictions on the validation set
y_val_pred = nb_classifier.predict(X_val_tfidf)

# Evaluate the model on the validation set
classification_rep = classification_report(y_val, y_val_pred)
cm = confusion_matrix(y_val, y_val_pred)

# Print the classification report and confusion matrix
print('Classification Report:\n', classification_rep)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

### **Accuracy and General Performance**

-   The model achieved an accuracy of 90%, which indicates a relatively high level of accuracy in predictions. However, this is based on a small test, which might not be sufficient to generalize the model’s performance to unseen data.

### **Classification Report on Validation Data**

-   **Precision**: For the negative class (0), it's 0.87, and for the positive class (1), it's 1.00.

-   **Recall**: For the negative class, it's 1.00, and for the positive class, it's 0.67.

-   **F1-Score**: For the negative class, it's 0.93, and for the positive class, it's 0.80.

-   **Accuracy**: The overall accuracy of the model on the validation set is 90%.

### **Confusion Matrix**

-   The confusion matrix reveals that the model made very few mistakes, with only zero false positive and seven false negative. However, due to the small size of the test set, each mistake has a significant impact on the performance metrics.

### **Conclusion**

-   The slight drop in accuracy from training to validation indicates that the model might be overfitting to the training data. However, the drop is not very large, suggesting that the overfitting is mild.
-   The model is performing well on both the training and validation sets, showing its ability to generalize to unseen data, although there's room for improvement, especially in capturing more of the positive class (related to nuclear energy).