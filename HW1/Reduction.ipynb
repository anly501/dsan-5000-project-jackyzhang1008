{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Reduction\"\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "5ca2f5db"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensionality Reduction with PCA\n",
        "\n",
        "This data set is about radiation reading in different state around the US, and the columns are LOCATION_NAME, SAMPLE COLLECTION TIME,DOSE EQUIVALENT RATE (nSv/h),GAMMA COUNT RATE R02 (CPM),GAMMA COUNT RATE R03 (CPM),GAMMA COUNT RATE R04 (CPM),GAMMA COUNT RATE R05 (CPM),GAMMA COUNT RATE R06 (CPM),GAMMA COUNT RATE R07 (CPM),GAMMA COUNT RATE R08 (CPM),GAMMA COUNT RATE R09 (CPM),STATUS.\n",
        "\n",
        "Most of the columns are not useful so I just kelp LOCATION_NAME and DOSE EQUIVALENT RATE (nSv/h).\n"
      ],
      "id": "3de93e54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "dddcfa9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv('../data/Cleaned/merged_data.csv')\n",
        "#data = data = data.sample(n=1000)\n",
        "# One-hot encoding the 'LOCATION_NAME' column\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "location_encoded = encoder.fit_transform(data[['LOCATION_NAME']])\n",
        "\n",
        "# Create a DataFrame from the encoded columns\n",
        "location_encoded_df = pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['LOCATION_NAME']))\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "dose_scaled = scaler.fit_transform(data[['DOSE EQUIVALENT RATE (nSv/h)']])\n",
        "\n",
        "# Create a DataFrame from the scaled column\n",
        "dose_scaled_df = pd.DataFrame(dose_scaled, columns=['DOSE EQUIVALENT RATE (nSv/h)'])\n",
        "preprocessed_data = pd.concat([location_encoded_df, dose_scaled_df], axis=1)\n",
        "preprocessed_data.head()\n",
        "\n",
        "# Applying PCA\n",
        "pca = PCA()\n",
        "pca.fit(preprocessed_data)\n",
        "\n",
        "# Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "pca_2 = PCA(n_components=2)\n",
        "pca_2 = pca_2.fit_transform(preprocessed_data)\n",
        "\n",
        "# Creating a DataFrame for the PCA results\n",
        "pca_df = pd.DataFrame(data=pca_2, columns=['PC1', 'PC2'])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(pca_df['PC1'], pca_df['PC2'])\n",
        "plt.title('PCA: First Two Principal Components')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "6be85d53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first plot shows a curve that flattens out as the number of components increases, which indicates that each additional component accounts for a smaller increment in the explained variance. The 'elbow' of the plot is the point where the explained variance starts to increase at a slower rate, and it is often used to decide on the number of components to use for further analysis. In this plot, the elbow appears somewhat subtle but seems to occur around 20-30 components.\n",
        "\n",
        "The second plot is a scatter plot of the first two principal components derived from PCA. This plot is used to visualize the data in the reduced-dimensional space created by PCA. Points that are close together are similar with respect to the principal components, and clusters may represent groups with similar properties.\n",
        "\n",
        "# Dimensionality Reduction with t-SNE\n"
      ],
      "id": "8702fcd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-hot encode the \"LOCATION_NAME\" column\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "location_encoded = encoder.fit_transform(data[['LOCATION_NAME']])\n",
        "\n",
        "# Combine the one-hot encoded location data with the dose equivalent rate data\n",
        "combined_data = pd.concat([pd.DataFrame(location_encoded), data['DOSE EQUIVALENT RATE (nSv/h)']], axis=1)\n",
        "\n",
        "# Device configuration - for Apple GPU\n",
        "device = torch.device('mps')\n",
        "\n",
        "# Convert the combined data to a PyTorch tensor\n",
        "data_tensor = torch.tensor(combined_data.values, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "# Define a function to run t-SNE with different perplexity values and visualize the results\n",
        "def run_tsne_and_visualize(data, perplexities, n_iter=1000):\n",
        "    for perplexity in perplexities:\n",
        "        # Apply t-SNE\n",
        "        tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=42,n_jobs=-1).fit_transform(data)\n",
        "\n",
        "        # Plot the results\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], alpha=0.5)\n",
        "        plt.title(f't-SNE with Perplexity = {perplexity}')\n",
        "        plt.xlabel('t-SNE Dimension 1')\n",
        "        plt.ylabel('t-SNE Dimension 2')\n",
        "        plt.show()\n",
        "\n",
        "# List of perplexity values to explore\n",
        "perplexities = [1, 5, 10]\n",
        "\n",
        "# Run and visualize t-SNE for each perplexity\n",
        "run_tsne_and_visualize(data_tensor.cpu().numpy(), perplexities)"
      ],
      "id": "01df6427",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For T-SNE, I was only able to perplexities in 1, 5, 10, as my computer is not able to process the data in higher perplexities.\n",
        "\n",
        "1.  **Perplexity = 1**: The third plot has a perplexity of 1, which is quite low and generally leads to focusing on very local structure. This can sometimes be too extreme, resulting in a loss of the bigger picture of the data structure. This plot shows a very disjointed clustering pattern, which suggests that the algorithm is perhaps too focused on local data variance and not capturing meaningful clusters effectively.\n",
        "\n",
        "2.  **Perplexity = 5**: The second plot is set with a perplexity of 5. Reducing the perplexity tends to make the t-SNE plot focus more on local structure, which often results in more fragmented clusters. Here, the clusters might start to become more separated compared to the first plot, but there is still a considerable blend across the data points.\n",
        "\n",
        "3.  **Perplexity = 10**: The first plot has a perplexity setting of 10. Perplexity is a parameter in t-SNE that roughly measures how to balance attention between local and global aspects of your data and can be interpreted as the effective number of neighbors. The choice of perplexity can affect the resulting plot significantly. This particular plot shows clusters that are not very distinct, indicating that the data points have a fair amount of overlap in the way they're grouped.\n",
        "\n",
        "# Evaluation and Comparison\n",
        "\n",
        "## **PCA:**\n",
        "\n",
        "-   **Linear Relationships:** PCA is effective at preserving linear relationships between variables. It projects data onto orthogonal axes (principal components) that maximize variance.\n",
        "\n",
        "-   **Global Structure Preservation:** It maintains the global structure of the data, making it suitable for capturing the overall distribution and relative distances between high-variance points.\n",
        "\n",
        "-   **Information Loss:** Some information, especially about low-variance features, may be lost in the dimensionality reduction process.\n",
        "\n",
        "## **t-SNE:**\n",
        "\n",
        "-   **Non-linear Relationships:** t-SNE excels at preserving local structures and non-linear relationships. It focuses on maintaining the similarity between nearby points, often revealing clusters or groups in the data.\n",
        "\n",
        "-   **Global vs. Local Structure:** While excellent at revealing local patterns, t-SNE might not accurately represent the global structure. Distances between clusters can be misleading.\n",
        "\n",
        "-   **Density Preservation:** It tends to preserve the density of the data, which can highlight substructures within clusters.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "-   **PCA** is suitable for large-scale exploratory analysis and when linear relationships are sufficient. Its interpretability and efficiency make it a standard choice for preliminary data analysis.\n",
        "\n",
        "-   **t-SNE** is more effective for detailed, local structure discovery in smaller datasets, particularly for revealing hidden clusters and patterns that PCA might miss.\n",
        "\n",
        "# Reference\n",
        "\n",
        "[@usepa2021]"
      ],
      "id": "8d5915ab"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}