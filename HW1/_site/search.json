[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Jiachen Zhang, I graduated from Rutgers University this May with a double major in computer science and statistics. But I did not start with those majors in the first place. Instead, I started with a major in business. As I went on the business journey, I realized how statistics is the key to many industries and how powerful of a tool it can be in modern society. Moreover, as I started my class in statistics, I got more intrigued by how those methods can make a simile of useless data to tell a story and create a connection between different variables. As I was learning statistics, I discovered that coding plays an essential role in advancing statistics and data processing; as a result, I started computer science as a junior in college. I started late for a major and had to use my summer vacation to finish all required classes.\nGUID: jz922 GU Email: jz922@georgetown.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HW1",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Introduction.html#radioecology-nuclear-energy-and-the-environment",
    "href": "Introduction.html#radioecology-nuclear-energy-and-the-environment",
    "title": "Nuclear Energy",
    "section": "Radioecology: nuclear energy and the environment",
    "text": "Radioecology: nuclear energy and the environment\nRadioecology is the field of study in which elements of physical and biological sciences are combined to pursue knowledge of radioactivity in the environment, including movement of radioactive materials and the effects of ionizing radiation on populations and on ecological organization. Volume I contains chapters on ecological principles, radiological principles, environmental radioactivity and radionuclide behavior in ecosystems. Chapter Four on environmental radioactivity characterizes sources, types, and amounts of material and man-made radioactive materials and radiation in the environment. Concepts of radionuclide behavior in ecosystems and behavior (movements and concentrations) of several important element groups in selected ecosystems are surveyed. A chapter on methods used for quantitative predictions of radionuclide transport in the environment gives a reasonably complete treatment to environmental transport processes, radionuclide kinetics in ecosystem compartments, and the use of transport models. Two chapters summarize the known effects of ionizing radiation on species, populations, and higher levels of ecological organization. This information is put into perspective in terms of risk and other consequences. Both volumes are well referenced. Appendices contain listings of major proceedings and books, reviews of specific radionuclides in the environment, and physical data used to characterize radionuclides. Radioecology: nuclear energy and the environment"
  },
  {
    "objectID": "Introduction.html#materials-for-future-nuclear-energy-systems",
    "href": "Introduction.html#materials-for-future-nuclear-energy-systems",
    "title": "Nuclear Energy",
    "section": "Materials for future nuclear energy systems",
    "text": "Materials for future nuclear energy systems\nMaterials for future nuclear energy systems must operate under more extreme conditions than those in current Gen II or Gen III systems. These conditions include higher temperature, higher dpa, and more corrosive environments. This paper reviews some of the fuels and structural materials used in advanced nuclear energy systems and identifies promising candidates for these systems. Fuel systems includes metallic fuels for the sodium cooled reactor, TRISO-coated particle fuel for the high temperature gas reactor, molten salt reactor fuels, and accident tolerant fuels for light water reactors. Structural materials applications include the sodium fast reactor, lead fast reactor, high temperature gas reactor, molten salt reactor and extended life light water reactors. A final section focuses on plasma-facing and blanket materials for deuterium-tritium fusion reactors.Materials for future nuclear energy systems"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Nuclear Energy",
    "section": "",
    "text": "What is nuclear energy?\nWhy are we using nuclear energy?\nAdvantages of nuclear energy?\nDisadvantages of nuclear energy?\nHow to radioactive waste is managed?\nIs nuclear energy safe?\nIs there any environmental problem with nuclear energy?\nWhat is the difference between fission and fusion?\nHow do nuclear power plant work?\nWhat is the future for nuclear power?"
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "jackyzhang1008\n\n\n\nThis is my GitHub"
  },
  {
    "objectID": "Code.html#username",
    "href": "Code.html#username",
    "title": "Code",
    "section": "",
    "text": "jackyzhang1008"
  },
  {
    "objectID": "Code.html#repo",
    "href": "Code.html#repo",
    "title": "Code",
    "section": "",
    "text": "This is my GitHub"
  },
  {
    "objectID": "Gathering.html",
    "href": "Gathering.html",
    "title": "Gathering",
    "section": "",
    "text": "There are 9 .xlsx file that shows the nuclear energy utilization for power plants in from 2013 to 2022. Within each table there are 20 columns.\nState: US State\nPlant ID: unique ID for a Nuclear power plant\nPlant Name: Nuclear power plant name\nUnit: reactor unit number\nJanuary - December: generating of electricity in Megawatthours\nYear_to_Date: generating of electricity in Megawatthours in the entire year\nNameplate:\nSummer:\nUtilization Factor: Summer/Nameplate, this shows how much of the power generated are been used\nData website\n\n\n\n\n# From https://www.eia.gov/nuclear/generation/index.html\nimport pandas as pd\nimport numpy as np\n#pd read xlsx\nusreact13 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact13.xlsx')\nusreact14 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact14.xlsx')\nusreact15 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact15.xlsx')\nusreact16 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact16.xlsx')\nusreact17 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact17.xlsx')\nusreact18 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact18.xlsx')\nusreact19 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact19.xlsx')\nusreact20 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact20.xlsx')\nusreact21 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact21.xlsx')\nusreact22 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact22.xlsx')"
  },
  {
    "objectID": "Cleaning.html",
    "href": "Cleaning.html",
    "title": "Cleaning",
    "section": "",
    "text": "# From https://www.eia.gov/nuclear/generation/index.html\nimport pandas as pd\nimport numpy as np\n#pd read xlsx\nusreact13 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact13.xlsx')\nusreact14 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact14.xlsx')\nusreact15 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact15.xlsx')\nusreact16 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact16.xlsx')\nusreact17 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact17.xlsx')\nusreact18 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact18.xlsx')\nusreact19 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact19.xlsx')\nusreact20 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact20.xlsx')\nusreact21 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact21.xlsx')\nusreact22 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact22.xlsx')\n\n# create a function make row 5 as column names and remove row 0-4 and reset index and drop colum 4-17\ndef make_header(df):\n    df.columns = df.iloc[4]\n    df = df.drop(df.index[0:5])\n    df = df.reset_index(drop=True)\n    df = df.drop(df.columns[4:17], axis=1)\n    return df\n\n# apply the function to all dataframes\nusreact13 = make_header(usreact13)\nusreact14 = make_header(usreact14)\nusreact15 = make_header(usreact15)\nusreact16 = make_header(usreact16)\nusreact17 = make_header(usreact17)\nusreact18 = make_header(usreact18)\nusreact19 = make_header(usreact19)\nusreact20 = make_header(usreact20)\nusreact21 = make_header(usreact21)\nusreact22 = make_header(usreact22)\n\n# create a function to replace \".\" and empty space with NaN, drop \"Plant ID\" column and rename \"Plant Name\" to \"Plant\", make colum \"Nameplate\",\"Summer\", \"Utilization Factor\" as numeric\ndef clean_df(df):\n    df = df.replace(r'^\\s*$', np.nan, regex=True)\n    df = df.drop(columns=['Plant ID'])\n    df = df.rename(columns={'Plant Name':'Plant'})\n    df['Nameplate'] = pd.to_numeric(df['Nameplate'], errors='coerce')\n    df['Summer'] = pd.to_numeric(df['Summer'], errors='coerce')\n    df['Utilization Factor'] = pd.to_numeric(df['Utilization Factor'], errors='coerce')\n    return df\n\n# apply the function to all dataframes\nusreact13 = clean_df(usreact13)\nusreact14 = clean_df(usreact14)\nusreact15 = clean_df(usreact15)\nusreact16 = clean_df(usreact16)\nusreact17 = clean_df(usreact17)\nusreact18 = clean_df(usreact18)\nusreact19 = clean_df(usreact19)\nusreact20 = clean_df(usreact20)\nusreact21 = clean_df(usreact21)\nusreact22 = clean_df(usreact22)\n\n# drop rows that colum \"State\" end with \"Total\" and reset index\ndef drop_total(df):\n    df = df[~df['State'].str.contains(\"Total\")]\n    df = df.reset_index(drop=True)\n    df = df.drop(columns=['Unit ID'])\n    return df\n\n# apply the function to all dataframes\nusreact13 = drop_total(usreact13)\nusreact14 = drop_total(usreact14)\nusreact15 = drop_total(usreact15)\nusreact16 = drop_total(usreact16)\nusreact17 = drop_total(usreact17)\nusreact18 = drop_total(usreact18)\nusreact19 = drop_total(usreact19)\nusreact20 = drop_total(usreact20)\nusreact21 = drop_total(usreact21)\nusreact22 = drop_total(usreact22)\n\n# drop state column, Nameplate, Summer than group by plant and average the data to one decimal place and reset index\ndef group_plant(df):\n    df = df.drop(columns=['State', 'Nameplate', 'Summer'])\n    df = df.groupby(['Plant'], as_index=False).mean().round(1)\n    df = df.reset_index(drop=True)\n    return df\n# apply the function to all dataframes\nusreact13 = group_plant(usreact13)\nusreact14 = group_plant(usreact14)\nusreact15 = group_plant(usreact15)\nusreact16 = group_plant(usreact16)\nusreact17 = group_plant(usreact17)\nusreact18 = group_plant(usreact18)\nusreact19 = group_plant(usreact19)\nusreact20 = group_plant(usreact20)\nusreact21 = group_plant(usreact21)\nusreact22 = group_plant(usreact22)\n\n# merge all dataframes based on \"Plant\" column, create new columns 2013-2022 and fill in the data\nusreact = pd.merge(usreact13, usreact14, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor_x':'2013', 'Utilization Factor_y':'2014'})\nusreact = pd.merge(usreact, usreact15, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2015'})\nusreact = pd.merge(usreact, usreact16, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2016'})\nusreact = pd.merge(usreact, usreact17, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2017'})\nusreact = pd.merge(usreact, usreact18, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2018'})\nusreact = pd.merge(usreact, usreact19, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2019'})\nusreact = pd.merge(usreact, usreact20, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2020'})\nusreact = pd.merge(usreact, usreact21, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2021'})\nusreact = pd.merge(usreact, usreact22, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2022'})\n\n# drop rows with NaN values and reset index\nusreact = usreact.dropna()\nusreact = usreact.reset_index(drop=True)\n\n# output the dataframe to csv\n#usreact.to_csv('Nuclear_Energy_Utilization_Factor.csv', index=False)"
  },
  {
    "objectID": "Cleaning.html#cleaning-1",
    "href": "Cleaning.html#cleaning-1",
    "title": "Cleaning",
    "section": "Cleaning:",
    "text": "Cleaning:\n\n# https://www.eia.gov/opendata/browser/electricity/electric-power-operational-data?frequency=annual&data=consumption-for-eg-btu;total-consumption-btu;&facets=fueltypeid;&fueltypeid=NUC;&start=2013&end=2022&sortColumn=period;&sortDirection=desc;\n# API_Key = \"Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\"\nimport requests\nimport pandas as pd\nimport numpy as np \nimport json\nimport csv\nthor_url = \"https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=annual&data[0]=consumption-for-eg-btu&data[1]=total-consumption-btu&facets[fueltypeid][]=NUC&start=2013&end=2022&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\"\nresp = requests.get(thor_url)\ndata = resp.text\ndata = json.loads(data)\ndetails=data['response']['data']\nrows=[]\nfor chunk in details:\n    row=[]\n    for key,value in chunk.items():\n        value=value if value else np.nan\n        row.append(value)\n        # print('row',row)\n    rows.append(row)\n\n# Columns\ncolumns = ['period', 'location', 'stateDescription', 'sectorid', 'sectorDescription', 'fueltypeid', 'fuelTypeDescription', 'consumption-for-eg-btu', 'consumption-for-eg-btu-units', 'total-consumption-btu', 'total-consumption-btu-units']\n\n# import to pandas dataframe\ndf = pd.DataFrame(rows, columns=columns)\ndf = df.drop(columns=['consumption-for-eg-btu-units', 'total-consumption-btu-units', 'fueltypeid', 'location', 'fuelTypeDescription', 'consumption-for-eg-btu', 'sectorid', 'stateDescription'])\n# export to csv\n#df.to_csv('EIA_API_py.csv', index=False)"
  },
  {
    "objectID": "Cleaning.html#link-1",
    "href": "Cleaning.html#link-1",
    "title": "Cleaning",
    "section": "Link:",
    "text": "Link:"
  },
  {
    "objectID": "Cleaning.html#cleaning-2",
    "href": "Cleaning.html#cleaning-2",
    "title": "Cleaning",
    "section": "Cleaning:",
    "text": "Cleaning:\n# https://www.eia.gov/opendata/browser/electricity/electric-power-operational-data?frequency=annual&data=consumption-for-eg-btu;total-consumption-btu;&facets=fueltypeid;&fueltypeid=COW;&start=2013&end=2022&sortColumn=period;&sortDirection=desc;\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nres &lt;- GET(\"https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=annual&data[0]=consumption-for-eg-btu&data[1]=total-consumption-btu&facets[fueltypeid][]=COW&start=2013&end=2022&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\")\ndata &lt;- fromJSON(rawToChar(res$content))\ndata &lt;- data$response\nperiod &lt;- c(data$data$period)\nlocation &lt;- c(data$data$location)\nstateDescription &lt;- c(data$data$stateDescription)\nsectorid &lt;- c(data$data$sectorid)\nsectorDescription &lt;- c(data$data$sectorDescription)\ntotal_consumption_btu &lt;- c(data$data$`total-consumption-btu`)\ndf &lt;- data.frame(period, location, stateDescription, sectorid, sectorDescription, total_consumption_btu)\nwrite.csv(df, \"/Users/jackyzhang/Documents/Python/DSAN-5000/HW/HW2/Part2/Code/EIA_API_r.csv\", row.names=FALSE)"
  },
  {
    "objectID": "Cleaning.html#link-2",
    "href": "Cleaning.html#link-2",
    "title": "Cleaning",
    "section": "Link:",
    "text": "Link:"
  },
  {
    "objectID": "Cleaning.html#cleaning",
    "href": "Cleaning.html#cleaning",
    "title": "Cleaning",
    "section": "",
    "text": "# From https://www.eia.gov/nuclear/generation/index.html\nimport pandas as pd\nimport numpy as np\n#pd read xlsx\nusreact13 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact13.xlsx')\nusreact14 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact14.xlsx')\nusreact15 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact15.xlsx')\nusreact16 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact16.xlsx')\nusreact17 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact17.xlsx')\nusreact18 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact18.xlsx')\nusreact19 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact19.xlsx')\nusreact20 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact20.xlsx')\nusreact21 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact21.xlsx')\nusreact22 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact22.xlsx')\n\n# create a function make row 5 as column names and remove row 0-4 and reset index and drop colum 4-17\ndef make_header(df):\n    df.columns = df.iloc[4]\n    df = df.drop(df.index[0:5])\n    df = df.reset_index(drop=True)\n    df = df.drop(df.columns[4:17], axis=1)\n    return df\n\n# apply the function to all dataframes\nusreact13 = make_header(usreact13)\nusreact14 = make_header(usreact14)\nusreact15 = make_header(usreact15)\nusreact16 = make_header(usreact16)\nusreact17 = make_header(usreact17)\nusreact18 = make_header(usreact18)\nusreact19 = make_header(usreact19)\nusreact20 = make_header(usreact20)\nusreact21 = make_header(usreact21)\nusreact22 = make_header(usreact22)\n\n# create a function to replace \".\" and empty space with NaN, drop \"Plant ID\" column and rename \"Plant Name\" to \"Plant\", make colum \"Nameplate\",\"Summer\", \"Utilization Factor\" as numeric\ndef clean_df(df):\n    df = df.replace(r'^\\s*$', np.nan, regex=True)\n    df = df.drop(columns=['Plant ID'])\n    df = df.rename(columns={'Plant Name':'Plant'})\n    df['Nameplate'] = pd.to_numeric(df['Nameplate'], errors='coerce')\n    df['Summer'] = pd.to_numeric(df['Summer'], errors='coerce')\n    df['Utilization Factor'] = pd.to_numeric(df['Utilization Factor'], errors='coerce')\n    return df\n\n# apply the function to all dataframes\nusreact13 = clean_df(usreact13)\nusreact14 = clean_df(usreact14)\nusreact15 = clean_df(usreact15)\nusreact16 = clean_df(usreact16)\nusreact17 = clean_df(usreact17)\nusreact18 = clean_df(usreact18)\nusreact19 = clean_df(usreact19)\nusreact20 = clean_df(usreact20)\nusreact21 = clean_df(usreact21)\nusreact22 = clean_df(usreact22)\n\n# drop rows that colum \"State\" end with \"Total\" and reset index\ndef drop_total(df):\n    df = df[~df['State'].str.contains(\"Total\")]\n    df = df.reset_index(drop=True)\n    df = df.drop(columns=['Unit ID'])\n    return df\n\n# apply the function to all dataframes\nusreact13 = drop_total(usreact13)\nusreact14 = drop_total(usreact14)\nusreact15 = drop_total(usreact15)\nusreact16 = drop_total(usreact16)\nusreact17 = drop_total(usreact17)\nusreact18 = drop_total(usreact18)\nusreact19 = drop_total(usreact19)\nusreact20 = drop_total(usreact20)\nusreact21 = drop_total(usreact21)\nusreact22 = drop_total(usreact22)\n\n# drop state column, Nameplate, Summer than group by plant and average the data to one decimal place and reset index\ndef group_plant(df):\n    df = df.drop(columns=['State', 'Nameplate', 'Summer'])\n    df = df.groupby(['Plant'], as_index=False).mean().round(1)\n    df = df.reset_index(drop=True)\n    return df\n# apply the function to all dataframes\nusreact13 = group_plant(usreact13)\nusreact14 = group_plant(usreact14)\nusreact15 = group_plant(usreact15)\nusreact16 = group_plant(usreact16)\nusreact17 = group_plant(usreact17)\nusreact18 = group_plant(usreact18)\nusreact19 = group_plant(usreact19)\nusreact20 = group_plant(usreact20)\nusreact21 = group_plant(usreact21)\nusreact22 = group_plant(usreact22)\n\n# merge all dataframes based on \"Plant\" column, create new columns 2013-2022 and fill in the data\nusreact = pd.merge(usreact13, usreact14, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor_x':'2013', 'Utilization Factor_y':'2014'})\nusreact = pd.merge(usreact, usreact15, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2015'})\nusreact = pd.merge(usreact, usreact16, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2016'})\nusreact = pd.merge(usreact, usreact17, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2017'})\nusreact = pd.merge(usreact, usreact18, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2018'})\nusreact = pd.merge(usreact, usreact19, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2019'})\nusreact = pd.merge(usreact, usreact20, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2020'})\nusreact = pd.merge(usreact, usreact21, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2021'})\nusreact = pd.merge(usreact, usreact22, on='Plant', how='outer')\nusreact = usreact.rename(columns={'Utilization Factor':'2022'})\n\n# drop rows with NaN values and reset index\nusreact = usreact.dropna()\nusreact = usreact.reset_index(drop=True)\n\n# output the dataframe to csv\n#usreact.to_csv('Nuclear_Energy_Utilization_Factor.csv', index=False)"
  },
  {
    "objectID": "Cleaning.html#link",
    "href": "Cleaning.html#link",
    "title": "Cleaning",
    "section": "Link:",
    "text": "Link:"
  },
  {
    "objectID": "Cleaning.html#result-1",
    "href": "Cleaning.html#result-1",
    "title": "Cleaning",
    "section": "Result:",
    "text": "Result:"
  },
  {
    "objectID": "Cleaning.html#result-2",
    "href": "Cleaning.html#result-2",
    "title": "Cleaning",
    "section": "Result:",
    "text": "Result:"
  },
  {
    "objectID": "Gathering.html#description-of-the-data",
    "href": "Gathering.html#description-of-the-data",
    "title": "Gathering",
    "section": "",
    "text": "There are 9 .xlsx file that shows the nuclear energy utilization for power plants in from 2013 to 2022. Within each table there are 20 columns.\nState: US State\nPlant ID: unique ID for a Nuclear power plant\nPlant Name: Nuclear power plant name\nUnit: reactor unit number\nJanuary - December: generating of electricity in Megawatthours\nYear_to_Date: generating of electricity in Megawatthours in the entire year\nNameplate:\nSummer:\nUtilization Factor: Summer/Nameplate, this shows how much of the power generated are been used\nData website"
  },
  {
    "objectID": "Gathering.html#use-excel-to-get-the-data",
    "href": "Gathering.html#use-excel-to-get-the-data",
    "title": "Gathering",
    "section": "",
    "text": "# From https://www.eia.gov/nuclear/generation/index.html\nimport pandas as pd\nimport numpy as np\n#pd read xlsx\nusreact13 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact13.xlsx')\nusreact14 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact14.xlsx')\nusreact15 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact15.xlsx')\nusreact16 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact16.xlsx')\nusreact17 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact17.xlsx')\nusreact18 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact18.xlsx')\nusreact19 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact19.xlsx')\nusreact20 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact20.xlsx')\nusreact21 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact21.xlsx')\nusreact22 = pd.read_excel('../Data/Pre-Clean/Nuclear_Generation/usreact22.xlsx')"
  },
  {
    "objectID": "Gathering.html#description-of-the-data-1",
    "href": "Gathering.html#description-of-the-data-1",
    "title": "Gathering",
    "section": "Description of the data",
    "text": "Description of the data\nperiod: years 2013-2022\nlocation: US state, US\nstateDescription: description of the location\nsectorid: unique ID for planet\nsectorDescription: planet name\nfueltypeid: all nuclear\nfuelTypeDescription: all nuclear\nconsumption-for-eg-btu: fuel usage\nconsumption-for-eg-btu-units: units in million MMBtu\ntotal-consumption-btu: total fuel usage\ntotal-consumption-btu-units: units in million MMBtu\nData website"
  },
  {
    "objectID": "Gathering.html#use-python-api-to-get-the-data",
    "href": "Gathering.html#use-python-api-to-get-the-data",
    "title": "Gathering",
    "section": "Use python API to get the data",
    "text": "Use python API to get the data\n\n# https://www.eia.gov/opendata/browser/electricity/electric-power-operational-data?frequency=annual&data=consumption-for-eg-btu;total-consumption-btu;&facets=fueltypeid;&fueltypeid=NUC;&start=2013&end=2022&sortColumn=period;&sortDirection=desc;\n# API_Key = \"Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\"\nimport requests\nimport pandas as pd\nimport numpy as np \nimport json\nimport csv\nthor_url = \"https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=annual&data[0]=consumption-for-eg-btu&data[1]=total-consumption-btu&facets[fueltypeid][]=NUC&start=2013&end=2022&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\"\nresp = requests.get(thor_url)\ndata = resp.text\ndata = json.loads(data)\ndetails=data['response']['data']\nrows=[]\nfor chunk in details:\n    row=[]\n    for key,value in chunk.items():\n        value=value if value else np.nan\n        row.append(value)\n        # print('row',row)\n    rows.append(row)\n\n# Columns\ncolumns = ['period', 'location', 'stateDescription', 'sectorid', 'sectorDescription', 'fueltypeid', 'fuelTypeDescription', 'consumption-for-eg-btu', 'consumption-for-eg-btu-units', 'total-consumption-btu', 'total-consumption-btu-units']"
  },
  {
    "objectID": "Gathering.html#description-of-the-data-2",
    "href": "Gathering.html#description-of-the-data-2",
    "title": "Gathering",
    "section": "Description of the data",
    "text": "Description of the data\nperiod: years 2013-2022\nlocation: US state, US\nstateDescription: description of the location\nsectorid: unique ID for planet\nsectorDescription: planet name\nfueltypeid: all coal\nfuelTypeDescription: all coal\nconsumption-for-eg-btu: fuel usage\nconsumption-for-eg-btu-units: units in million MMBtu\ntotal-consumption-btu: total fuel usage\ntotal-consumption-btu-units: units in million MMBtu\nData website"
  },
  {
    "objectID": "Gathering.html#use-r-api-to-get-the-data",
    "href": "Gathering.html#use-r-api-to-get-the-data",
    "title": "Gathering",
    "section": "Use r API to get the data",
    "text": "Use r API to get the data\n# https://www.eia.gov/opendata/browser/electricity/electric-power-operational-data?frequency=annual&data=consumption-for-eg-btu;total-consumption-btu;&facets=fueltypeid;&fueltypeid=COW;&start=2013&end=2022&sortColumn=period;&sortDirection=desc;\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nres &lt;- GET(\"https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=annual&data[0]=consumption-for-eg-btu&data[1]=total-consumption-btu&facets[fueltypeid][]=COW&start=2013&end=2022&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=Bc2HtlspmpvpzuBbPfIi8HoancNTzVYN9YApx3fu\")\ndata &lt;- fromJSON(rawToChar(res$content))"
  },
  {
    "objectID": "Naïve_Bayes.html",
    "href": "Naïve_Bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a classification technique based on applying Bayes’ theorem with a strong (naive) assumption that all the features are independent of each other. Despite its simplicity, Naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering.\n\n\n\nThe model is trained on the training dataset using Bayes’ theorem, and the maximum likelihood method is used to estimate the parameters. To classify a new instance, the likelihood of that instance is calculated for each class, and the class with the highest likelihood is assigned to the instance.\n\n\n\n\\[\nP(C \\mid X)=\\frac{P(X \\mid C) \\cdot P(C)}{P(X)}\n\\]\nAs,\n\n\\(P(C \\mid X)\\) is the posterior probability of class \\(C\\) given predictor \\(X\\).\n\\(P(C)\\) is the prior probability of class.\n\\(P(X \\mid C)\\) is the likelihood which is the probability of predictor given class.\n\\(P(X)\\) is the prior probability of predictor.\n\n\n\n\nThe main objective of using Naive Bayes classification is to classify based on utilization of power plant in the future based on the observed characteristics, in this case will be the mean from 2013-2021; assuming the features are independent.\n\n\n\n\nPredict the 2022 utilization for each reactor based on the mean from 2013-2021, anything that is over 90 will satisfy as High utilization and below 90 will be Low utilization.\nAnalysis text data to determine if they are related to the key word: “nuclear”, “radiation”, “reactor”, “uranium”, and “plutonium”\n\n\n\n\n\nGaussian Naive Bayes: Assumes that the features follow a normal distribution. This variant is used in cases where continuous values are associated with each class.\nMultinomial Naive Bayes: It is used for discrete counts. For example, let’s say we have a text classification problem. Here we can consider the occurrence of words as an event. In such a case, we will use a Multinomial Naive Bayes classifier.\nBernoulli Naive Bayes: This is similar to the Multinomial Naive Bayes but the predictors are boolean variables. So, the parameters that are learned are the probabilities of the different categories and the probabilities of the predictors in each category. This variant is used when the feature vectors are binary (i.e., zeros and ones)."
  },
  {
    "objectID": "Naïve_Bayes.html#feature-selection-for-record-data",
    "href": "Naïve_Bayes.html#feature-selection-for-record-data",
    "title": "Naïve Bayes",
    "section": "Feature selection for record data",
    "text": "Feature selection for record data\n\n# Calculate the mean utilization factor from 2013 to 2021\ndf['Mean_Utilization_2013_2021'] = df.loc[:, '2013':'2021'].mean(axis=1)\n\n# Convert the utilization factor for 2022 into categories\ndf['Utilization_2022_Category'] = df['2022'].apply(lambda x: 'High' if x &gt;= 90 else 'Low')\n\n# Prepare the data\nX = df[['Mean_Utilization_2013_2021']]\ny = df['Utilization_2022_Category']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nThis part prepare data for"
  },
  {
    "objectID": "Naïve_Bayes.html#feature-selection-for-text-data",
    "href": "Naïve_Bayes.html#feature-selection-for-text-data",
    "title": "Naïve Bayes",
    "section": "Feature selection for text data",
    "text": "Feature selection for text data\nfor this part"
  },
  {
    "objectID": "Exploration.html",
    "href": "Exploration.html",
    "title": "Exploration",
    "section": "",
    "text": "Rows: Each row corresponds to a different nuclear plant.\nColumns: The columns include the plant’s name and its utilization factor for each year from 2013 to 2022.\nData Types: The “Plant” column is of object data type (string), and the rest of the columns (2013 to 2022) are numerical.\nMissing Values: We need to check for any missing values in the dataset.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Read in data\nnuclear_data = pd.read_csv('../data/Cleaned/EDA/Nuclear_Energy_Utilization_Factor.csv')\n\n# Checking for missing values\nmissing_values = nuclear_data.isnull().sum()\nprint(missing_values)\n\nPlant    0\n2013     0\n2014     0\n2015     0\n2016     0\n2017     0\n2018     0\n2019     0\n2020     0\n2021     0\n2022     0\ndtype: int64\n\n\n\n\n\nWe will calculate and report the following summary statistics for the numerical variables (utilization factor for the years 2013 to 2022):\n\nMean\nMedian\nMode\nStandard Deviation\nVariance\n\n\n# Calculating descriptive statistics for the numerical variables\ndescriptive_stats = nuclear_data.describe(include=[float]).T\ndescriptive_stats['mode'] = nuclear_data.mode().iloc[0, 1:]\ndescriptive_stats['variance'] = descriptive_stats['std']**2\n\n# Displaying the descriptive statistics\nprint(descriptive_stats[['mean', '50%', 'mode', 'std', 'variance']])\n\n        mean    50%  mode       std   variance\n2013  90.836  92.55  89.8  7.809381  60.986433\n2014  91.314  92.15  97.2  6.001123  36.013473\n2015  91.376  93.10  94.3  6.188091  38.292473\n2016  91.856  94.60  90.2  9.806769  96.172718\n2017  91.498  93.80  97.4  8.357314  69.844690\n2018  92.078  92.90  92.8  7.834078  61.372771\n2019  92.916  94.05  91.8  6.336366  40.149535\n2020  92.062  95.00  95.4  9.486788  89.999139\n2021  91.818  93.35  88.4  9.272643  85.981914\n2022  92.318  94.40  94.4  7.336815  53.828853\n\n\nHere are the descriptive statistics for the nuclear plants’ utilization factors from 2013 to 2022:\n\nMean: The average utilization factor for each year. It ranges from 90.836 in 2013 to 92.916 in 2019.\nMedian (50%): The middle value of the utilization factor for each year. It ranges from 92.55 in 2013 to 94.40 in 2022.\nMode: The value that appears most frequently in each year. It varies across years, with some years having more common utilization factors than others.\nStandard Deviation (std): Measures the amount of variation or dispersion from the average. A low standard deviation indicates that the values tend to be close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.\nVariance: The square of the standard deviation. It provides a measure of how spread out the values are.\n\n\n\n\nIn this section, we’ll create visualizations to further explore the data’s distribution, relationships between variables, and potential patterns or trends.\n\nHistograms: To visualize the distribution of utilization factors for each year.\nBox Plots: To observe the spread of the data and identify any potential outliers.\n\n\n\n\n# Setting the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Creating histograms for the utilization factors of each year\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Flattening the 2D array of axes for easy iteration\naxes = axes.flatten()\n\n# Plotting the histograms\nfor i, year in enumerate(range(2013, 2023)):\n    sns.histplot(nuclear_data[str(year)], bins=20, kde=True, ax=axes[i])\n    axes[i].set_title(f'Distribution in {year}')\n    axes[i].set_xlabel('Utilization Factor')\n    axes[i].set_ylabel('Frequency')\n\n# Adjusting layout for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe histograms above depict the distribution of utilization factors for each nuclear plant across different years. Here are some observations:\n\nThe distribution of utilization factors is fairly symmetric in most years, with a slight skewness in some years.\nThe majority of the nuclear plants seem to operate with a high utilization factor, mostly around 90 to 100.\nThere are a few years (e.g., 2016, 2020) where the distribution appears to be more spread out, indicating higher variability in the utilization factors.\n\n\n\n\n\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Flattening the 2D array of axes for easy iteration\naxes = axes.flatten()\n\n# Plotting the box plots\nfor i, year in enumerate(range(2013, 2023)):\n    sns.boxplot(x=nuclear_data[str(year)], ax=axes[i])\n    axes[i].set_title(f'Box Plot in {year}')\n    axes[i].set_xlabel('Utilization Factor')\n\n# Adjusting layout for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe box plots above provide a visual representation of the distribution of utilization factors for each year, highlighting the spread of the data and any potential outliers.\n\nThe median (represented by the line inside the box) is consistently high across all years, mostly above 90.\nThe interquartile range (IQR, represented by the height of the box) is relatively narrow, indicating that the middle 50% of the data points are close to each other.\nThere are some potential outliers in various years, particularly in 2016 and 2020, where the utilization factors are significantly lower than the rest of the data.\n\n\n\n\n\n\n# Calculating the correlation matrix\ncorrelation_matrix = nuclear_data.iloc[:, 1:].corr()\n\n# Creating a heatmap to visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nplt.title('Correlation Matrix of Utilization Factors (2013-2022)')\nplt.show()\n\n\n\n\n\nMost of the correlations between different years are positive, suggesting that a higher utilization factor in one year is generally associated with a higher utilization factor in another year.\nThe correlation coefficients are mostly in the range of 0.2 to 0.8, indicating a moderate positive relationship between the years.\nThere are some years with higher correlations to each other (e.g., 2017 and 2019 with a correlation of 0.84), suggesting a stronger relationship in performance between these years.\n\n\n\n\nBased on the observed patterns and relationships in the data, we might formulate hypotheses such as:\n\nH1: The utilization factor of a nuclear plant in one year is positively related to its utilization factor in the subsequent year.\nH2: There are certain nuclear plants that consistently perform better or worse than others across all years.\nH3: The variability in utilization factors has decreased or increased over the years.\n\n\n\n\n\n# Calculating the average utilization factor for each nuclear plant over the years\nnuclear_data['Average'] = nuclear_data.iloc[:, 1:].mean(axis=1)\n\n# Displaying the nuclear plants sorted by their average utilization factor\nsorted_plants = nuclear_data[['Plant', 'Average']].sort_values(by='Average', ascending=False)\nprint(sorted_plants)\n\n                                 Plant  Average\n32                        Peach Bottom    99.24\n6   Calvert Cliffs Nuclear Power Plant    98.61\n14          Dresden Generating Station    97.56\n23                            Limerick    97.48\n4             Byron Generating Station    97.41\n36      Quad Cities Generating Station    97.28\n2         Braidwood Generation Station    97.23\n28                              Oconee    96.19\n46                              Vogtle    95.82\n22          LaSalle Generating Station    95.60\n24                             McGuire    95.06\n34           Point Beach Nuclear Plant    94.76\n7                              Catawba    94.67\n26     Nine Mile Point Nuclear Station    94.46\n35                      Prairie Island    94.27\n37       R E Ginna Nuclear Power Plant    94.23\n43                               Surry    94.19\n39                            Seabrook    94.09\n1                        Beaver Valley    93.91\n29  PSEG Hope Creek Generating Station    93.65\n27                          North Anna    93.62\n8                Clinton Power Station    93.27\n19                              Harris    93.11\n15                       Edwin I Hatch    92.77\n41                 South Texas Project    92.73\n10                       Comanche Peak    92.57\n31                          Palo Verde    92.48\n11                         Davis Besse    92.38\n21                     Joseph M Farley    92.21\n33                               Perry    91.67\n44                        Turkey Point    91.17\n3                         Browns Ferry    91.13\n25                           Millstone    90.92\n18                        H B Robinson    90.59\n40                            Sequoyah    90.56\n13                       Donald C Cook    90.50\n42                            St Lucie    89.98\n20                 James A Fitzpatrick    89.85\n45                          V C Summer    89.73\n9          Columbia Generating Station    89.21\n12                       Diablo Canyon    89.13\n30       PSEG Salem Generating Station    88.34\n47                         Waterford 3    87.99\n38                          River Bend    87.07\n48             Watts Bar Nuclear Plant    85.61\n0                 Arkansas Nuclear One    85.13\n49       Wolf Creek Generating Station    84.91\n5                             Callaway    82.47\n16                               Fermi    79.67\n17                          Grand Gulf    73.88\n\n\n\n\nThe nuclear plants with the highest average utilization factors over the years are:\n\nPeach Bottom: 99.24\nCalvert Cliffs Nuclear Power Plant: 98.61\nDresden Generating Station: 97.56\nLimerick: 97.48\nByron Generating Station: 97.41\n\n\n\n\nThe nuclear plants with the lowest average utilization factors are:\n\nGrand Gulf: 73.88\nFermi: 79.67\nCallaway: 82.47\nWolf Creek Generating Station: 84.91\nArkansas Nuclear One: 85.13\n\n\n# Defining performance tiers based on the average utilization factor\nbins = [0, 80, 90, 95, 100]\nlabels = ['Low Performer (&lt;80)', 'Below Average (80-90)', 'Above Average (90-95)', 'High Performer (&gt;95)']\nnuclear_data['Performance Tier'] = pd.cut(nuclear_data['Average'], bins=bins, labels=labels, right=False)\n\n# Calculating the number of plants in each performance tier\nperformance_tiers = nuclear_data['Performance Tier'].value_counts().sort_index()\n\n# Displaying the number of plants in each performance tier\nprint(performance_tiers)\n\nPerformance Tier\nLow Performer (&lt;80)       2\nBelow Average (80-90)    12\nAbove Average (90-95)    25\nHigh Performer (&gt;95)     11\nName: count, dtype: int64\n\n\nThe nuclear plants have been categorized into different performance tiers based on their average utilization factor:\n\nLow Performer (&lt;80): 2 plants\nBelow Average (80-90): 12 plants\nAbove Average (90-95): 25 plants\nHigh Performer (&gt;95): 11 plants\n\n\n# Function to identify outliers using the IQR method\ndef identify_outliers(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = data[(data[column] &lt; (Q1 - 1.5 * IQR)) | (data[column] &gt; (Q3 + 1.5 * IQR))]\n    return outliers[['Plant', column]]\n\n# Identifying outliers for each year\noutliers_dict = {}\nfor year in range(2013, 2023):\n    outliers = identify_outliers(nuclear_data, str(year))\n    if not outliers.empty:\n        outliers_dict[year] = outliers\n\n# Displaying the outliers for each year\nprint(outliers_dict)\n\n{2013:                             Plant  2013\n0            Arkansas Nuclear One  73.5\n16                          Fermi  69.4\n49  Wolf Creek Generating Station  69.6, 2014:           Plant  2014\n11  Davis Besse  74.4, 2016:                             Plant  2016\n17                     Grand Gulf  47.9\n30  PSEG Salem Generating Station  76.8\n48        Watts Bar Nuclear Plant  62.6, 2017:                           Plant  2017\n9   Columbia Generating Station  78.8\n17                   Grand Gulf  60.0\n48      Watts Bar Nuclear Plant  69.0, 2018:          Plant  2018\n16       Fermi  74.2\n17  Grand Gulf  56.4, 2019:           Plant  2019\n38   River Bend  75.8\n47  Waterford 3  74.1, 2020:          Plant  2020\n5     Callaway  70.7\n16       Fermi  60.6\n17  Grand Gulf  52.6, 2021:          Plant  2021\n5     Callaway  39.3\n45  V C Summer  76.5, 2022:           Plant  2022\n16        Fermi  66.7\n17   Grand Gulf  70.1\n47  Waterford 3  77.0}\n\n\n\n\n\nOutliers have been identified for each year based on the IQR method. Here are the plants that were identified as outliers in each year:\n\n2013:\n\nArkansas Nuclear One (73.5)\nFermi (69.4)\nWolf Creek Generating Station (69.6)\n\n2014:\n\nDavis Besse (74.4)\n\n2016:\n\nGrand Gulf (47.9)\nPSEG Salem Generating Station (76.8)\nWatts Bar Nuclear Plant (62.6)\n\n2017:\n\nColumbia Generating Station (78.8)\nGrand Gulf (60.0)\nWatts Bar Nuclear Plant (69.0)\n\n2018:\n\nFermi (74.2)\nGrand Gulf (56.4)\n\n2019:\n\nRiver Bend (75.8)\nWaterford 3 (74.1)\n\n2020:\n\nCallaway (70.7)\nFermi (60.6)\nGrand Gulf (52.6)\n\n2021:\n\nCallaway (39.3)\nV C Summer (76.5)\n\n2022:\n\nFermi (66.7)\nGrand Gulf (70.1)\nWaterford 3 (77.0)\n\n\n\n\n\n\nThrough the exploratory data analysis of the nuclear energy utilization dataset, we gained valuable insights into the performance of various nuclear plants over a span of 10 years, from 2013 to 2022. Below is a summary of the key findings and insights:\n\n\n\nThe majority of the nuclear plants have high utilization factors, mostly ranging from 90 to 100.\nThe distribution of utilization factors is fairly symmetric in most years, though there is some variability and a few years show a wider spread of values.\n\n\n\n\n\nThe utilization factors across different years are generally positively correlated, suggesting that a plant’s performance in one year is related to its performance in other years.\n\n\n\n\n\nWe identified plants that consistently perform well, such as Peach Bottom, Calvert Cliffs Nuclear Power Plant, and Dresden Generating Station, with average utilization factors above 97.\nConversely, plants like Grand Gulf, Fermi, and Callaway were identified as low performers, with average utilization factors below 85.\n\n\n\n\n\nThe plants were categorized into performance tiers, with 11 plants identified as high performers, 25 as above average, 12 as below average, and 2 as low performers.\n\n\n\n\n\nOutliers were identified in each year, pointing to plants with significantly lower utilization factors compared to the rest.\nSome plants, such as Grand Gulf and Fermi, appeared as outliers in multiple years, indicating consistent issues that may require further investigation."
  },
  {
    "objectID": "Naïve_Bayes.html#record-data",
    "href": "Naïve_Bayes.html#record-data",
    "title": "Naïve Bayes",
    "section": "Record Data",
    "text": "Record Data\n\n# handling the outliers\n# Set any utilization factor in colum 2013-2022 above 100 to 100 in df\nimport numpy as np\nimport pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n\ndf = pd.read_csv('../Data/EDA/Nuclear_Energy_Utilization_Factor.csv')\nprint(df.describe())\ndf[df.columns[1:]] = df[df.columns[1:]].apply(lambda x: np.where(x &gt; 100, 100, x))\nprint(df.describe())\n\n             2013        2014        2015        2016        2017        2018  \\\ncount   50.000000   50.000000   50.000000   50.000000   50.000000   50.000000   \nmean    90.836000   91.314000   91.376000   91.856000   91.498000   92.078000   \nstd      7.809381    6.001123    6.188091    9.806769    8.357314    7.834078   \nmin     69.400000   74.400000   74.500000   47.900000   60.000000   56.400000   \n25%     87.725000   88.650000   86.925000   90.200000   89.650000   90.125000   \n50%     92.550000   92.150000   93.100000   94.600000   93.800000   92.900000   \n75%     96.775000   95.750000   96.125000   97.700000   96.600000   97.400000   \nmax    101.300000  101.000000  100.800000  106.500000  103.500000  102.200000   \n\n             2019        2020        2021        2022  \ncount   50.000000   50.000000   50.000000   50.000000  \nmean    92.916000   92.062000   91.818000   92.318000  \nstd      6.336366    9.486788    9.272643    7.336815  \nmin     74.100000   52.600000   39.300000   66.700000  \n25%     89.375000   90.100000   89.850000   89.600000  \n50%     94.050000   95.000000   93.350000   94.400000  \n75%     98.075000   97.625000   96.550000   97.550000  \nmax    100.400000  102.900000  100.200000  100.400000  \n             2013        2014        2015        2016        2017        2018  \\\ncount   50.000000   50.000000   50.000000   50.000000   50.000000   50.000000   \nmean    90.796000   91.294000   91.342000   91.696000   91.354000   92.000000   \nstd      7.758523    5.969768    6.137509    9.621807    8.185433    7.744438   \nmin     69.400000   74.400000   74.500000   47.900000   60.000000   56.400000   \n25%     87.725000   88.650000   86.925000   90.200000   89.650000   90.125000   \n50%     92.550000   92.150000   93.100000   94.600000   93.800000   92.900000   \n75%     96.775000   95.750000   96.125000   97.700000   96.600000   97.400000   \nmax    100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n\n             2019        2020        2021        2022  \ncount   50.000000   50.000000   50.000000   50.000000  \nmean    92.900000   91.962000   91.814000   92.296000  \nstd      6.317646    9.389136    9.268996    7.312666  \nmin     74.100000   52.600000   39.300000   66.700000  \n25%     89.375000   90.100000   89.850000   89.600000  \n50%     94.050000   95.000000   93.350000   94.400000  \n75%     98.075000   97.625000   96.550000   97.550000  \nmax    100.000000  100.000000  100.000000  100.000000  \n\n\nThis part is just like data cleaning, I am aiming to make sure that the utilization fill into the range from 0 to 100%. But in the real world there are cases where utilization goes over 100%, meaning that there could be more places that need more electrocites that the power plane can generate. For this case I am just going to make anything that is over 100% to 100%."
  },
  {
    "objectID": "Naïve_Bayes.html#record-data-1",
    "href": "Naïve_Bayes.html#record-data-1",
    "title": "Naïve Bayes",
    "section": "Record Data",
    "text": "Record Data\n\n# Calculate the mean utilization factor from 2013 to 2021\ndf['Mean_Utilization_2013_2021'] = df.loc[:, '2013':'2021'].mean(axis=1)\n\n# Convert the utilization factor for 2022 into categories\ndf['Utilization_2022_Category'] = df['2022'].apply(lambda x: 'High' if x &gt;= 90 else 'Low')\n\n# Prepare the data\nX = df[['Mean_Utilization_2013_2021']]\ny = df['Utilization_2022_Category']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nThis part prepares data for the Naive Bayes Classifier, as all the columns are concerning each other as they are just utilization reports of each year. Our goal for the Native Bayes Classifier is to find out the utilization rate of future years from that date of the year before."
  },
  {
    "objectID": "Naïve_Bayes.html#text-data",
    "href": "Naïve_Bayes.html#text-data",
    "title": "Naïve Bayes",
    "section": "Text Data",
    "text": "Text Data\nThis part show that …"
  },
  {
    "objectID": "Naïve_Bayes.html#text-data-1",
    "href": "Naïve_Bayes.html#text-data-1",
    "title": "Naïve Bayes",
    "section": "Text Data",
    "text": "Text Data\nfor this part"
  },
  {
    "objectID": "Naïve_Bayes.html#overview",
    "href": "Naïve_Bayes.html#overview",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a classification technique based on applying Bayes’ theorem with a strong (naive) assumption that all the features are independent of each other. Despite its simplicity, Naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering."
  },
  {
    "objectID": "Naïve_Bayes.html#how-it-works",
    "href": "Naïve_Bayes.html#how-it-works",
    "title": "Naïve Bayes",
    "section": "",
    "text": "The model is trained on the training dataset using Bayes’ theorem, and the maximum likelihood method is used to estimate the parameters. To classify a new instance, the likelihood of that instance is calculated for each class, and the class with the highest likelihood is assigned to the instance."
  },
  {
    "objectID": "Naïve_Bayes.html#bayes-theorem",
    "href": "Naïve_Bayes.html#bayes-theorem",
    "title": "Naïve Bayes",
    "section": "",
    "text": "\\[\nP(C \\mid X)=\\frac{P(X \\mid C) \\cdot P(C)}{P(X)}\n\\]\nAs,\n\n\\(P(C \\mid X)\\) is the posterior probability of class \\(C\\) given predictor \\(X\\).\n\\(P(C)\\) is the prior probability of class.\n\\(P(X \\mid C)\\) is the likelihood which is the probability of predictor given class.\n\\(P(X)\\) is the prior probability of predictor."
  },
  {
    "objectID": "Naïve_Bayes.html#objectives",
    "href": "Naïve_Bayes.html#objectives",
    "title": "Naïve Bayes",
    "section": "",
    "text": "The main objective of using Naive Bayes classification is to classify based on utilization of power plant in the future based on the observed characteristics, in this case will be the mean from 2013-2021; assuming the features are independent."
  },
  {
    "objectID": "Naïve_Bayes.html#variants-of-naive-bayes",
    "href": "Naïve_Bayes.html#variants-of-naive-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Gaussian Naive Bayes: Assumes that the features follow a normal distribution. This variant is used in cases where continuous values are associated with each class.\nMultinomial Naive Bayes: It is used for discrete counts. For example, let’s say we have a text classification problem. Here we can consider the occurrence of words as an event. In such a case, we will use a Multinomial Naive Bayes classifier.\nBernoulli Naive Bayes: This is similar to the Multinomial Naive Bayes but the predictors are boolean variables. So, the parameters that are learned are the probabilities of the different categories and the probabilities of the predictors in each category. This variant is used when the feature vectors are binary (i.e., zeros and ones)."
  },
  {
    "objectID": "Naïve_Bayes.html#aim-to-achieve",
    "href": "Naïve_Bayes.html#aim-to-achieve",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Predict the 2022 utilization for each reactor based on the mean from 2013-2021, anything that is over 90 will satisfy as High utilization and below 90 will be Low utilization.\nAnalysis text data to determine if they are related to the key word: “nuclear”, “radiation”, “reactor”, “uranium”, and “plutonium”"
  },
  {
    "objectID": "Naïve_Bayes.html#record-data-2",
    "href": "Naïve_Bayes.html#record-data-2",
    "title": "Naïve Bayes",
    "section": "Record Data",
    "text": "Record Data\n\n# Initialize the Gaussian Naive Bayes classifier\ngnb = GaussianNB()\n\n# Train the classifier\ngnb.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = gnb.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n        High       0.86      0.86      0.86         7\n         Low       0.67      0.67      0.67         3\n\n    accuracy                           0.80        10\n   macro avg       0.76      0.76      0.76        10\nweighted avg       0.80      0.80      0.80        10\n\n\n\n\n\n\n\nAccuracy and General Performance\n\nThe model achieved an accuracy of 80%, which indicates a relatively high level of accuracy in predictions. However, this is based on a small test set of 10 instances, which might not be sufficient to generalize the model’s performance to unseen data.\n\n\n\nPrecision and Recall\n\nThe model showed better precision and recall for the ‘High’ category compared to the ‘Low’ category. This indicates that the model is more reliable when predicting high utilization factors.\n\nHigh Utilization Category: Both precision and recall are 0.86, showing a balanced performance.\nLow Utilization Category: Both precision and recall are 0.67, which is lower compared to the ‘High’ category. This could be a point of improvement for the model.\n\n\n\n\nF1-Score\n\nThe F1-Score is also higher for the ‘High’ category (0.86) compared to the ‘Low’ category (0.67). Since F1-Score is the harmonic mean of precision and recall, this again highlights the model’s stronger performance in predicting high utilization factors.\n\n\n\nConfusion Matrix\n\nThe confusion matrix reveals that the model made very few mistakes, with only one false positive and one false negative. However, due to the small size of the test set, each mistake has a significant impact on the performance metrics.\n\n\n\nConclusion\n\nThe Gaussian Naive Bayes classifier provides a good starting point for predicting nuclear energy utilization categories based on historical data. The model demonstrates decent accuracy, but there is potential for enhancement, particularly in improving precision and recall for the “Low” category. The exploratory data analysis and visualizations offer valuable insights into the distribution and trends of utilization factors across different power plants. Future work could explore more sophisticated models, feature engineering, or additional data sources to further refine the predictions and gain deeper insights.​"
  },
  {
    "objectID": "Naïve_Bayes.html#text-data-2",
    "href": "Naïve_Bayes.html#text-data-2",
    "title": "Naïve Bayes",
    "section": "Text Data",
    "text": "Text Data"
  },
  {
    "objectID": "Exploration.html#data-import-and-understanding",
    "href": "Exploration.html#data-import-and-understanding",
    "title": "Exploration",
    "section": "",
    "text": "Rows: Each row corresponds to a different nuclear plant.\nColumns: The columns include the plant’s name and its utilization factor for each year from 2013 to 2022.\nData Types: The “Plant” column is of object data type (string), and the rest of the columns (2013 to 2022) are numerical.\nMissing Values: We need to check for any missing values in the dataset.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Read in data\nnuclear_data = pd.read_csv('../data/Cleaned/EDA/Nuclear_Energy_Utilization_Factor.csv')\n\n# Checking for missing values\nmissing_values = nuclear_data.isnull().sum()\nprint(missing_values)\n\nPlant    0\n2013     0\n2014     0\n2015     0\n2016     0\n2017     0\n2018     0\n2019     0\n2020     0\n2021     0\n2022     0\ndtype: int64"
  },
  {
    "objectID": "Exploration.html#descriptive-statistics",
    "href": "Exploration.html#descriptive-statistics",
    "title": "Exploration",
    "section": "",
    "text": "We will calculate and report the following summary statistics for the numerical variables (utilization factor for the years 2013 to 2022):\n\nMean\nMedian\nMode\nStandard Deviation\nVariance\n\n\n# Calculating descriptive statistics for the numerical variables\ndescriptive_stats = nuclear_data.describe(include=[float]).T\ndescriptive_stats['mode'] = nuclear_data.mode().iloc[0, 1:]\ndescriptive_stats['variance'] = descriptive_stats['std']**2\n\n# Displaying the descriptive statistics\nprint(descriptive_stats[['mean', '50%', 'mode', 'std', 'variance']])\n\n        mean    50%  mode       std   variance\n2013  90.836  92.55  89.8  7.809381  60.986433\n2014  91.314  92.15  97.2  6.001123  36.013473\n2015  91.376  93.10  94.3  6.188091  38.292473\n2016  91.856  94.60  90.2  9.806769  96.172718\n2017  91.498  93.80  97.4  8.357314  69.844690\n2018  92.078  92.90  92.8  7.834078  61.372771\n2019  92.916  94.05  91.8  6.336366  40.149535\n2020  92.062  95.00  95.4  9.486788  89.999139\n2021  91.818  93.35  88.4  9.272643  85.981914\n2022  92.318  94.40  94.4  7.336815  53.828853\n\n\nHere are the descriptive statistics for the nuclear plants’ utilization factors from 2013 to 2022:\n\nMean: The average utilization factor for each year. It ranges from 90.836 in 2013 to 92.916 in 2019.\nMedian (50%): The middle value of the utilization factor for each year. It ranges from 92.55 in 2013 to 94.40 in 2022.\nMode: The value that appears most frequently in each year. It varies across years, with some years having more common utilization factors than others.\nStandard Deviation (std): Measures the amount of variation or dispersion from the average. A low standard deviation indicates that the values tend to be close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.\nVariance: The square of the standard deviation. It provides a measure of how spread out the values are."
  },
  {
    "objectID": "Exploration.html#data-visualization",
    "href": "Exploration.html#data-visualization",
    "title": "Exploration",
    "section": "",
    "text": "In this section, we’ll create visualizations to further explore the data’s distribution, relationships between variables, and potential patterns or trends.\n\nHistograms: To visualize the distribution of utilization factors for each year.\nBox Plots: To observe the spread of the data and identify any potential outliers.\n\n\n\n\n# Setting the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Creating histograms for the utilization factors of each year\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Flattening the 2D array of axes for easy iteration\naxes = axes.flatten()\n\n# Plotting the histograms\nfor i, year in enumerate(range(2013, 2023)):\n    sns.histplot(nuclear_data[str(year)], bins=20, kde=True, ax=axes[i])\n    axes[i].set_title(f'Distribution in {year}')\n    axes[i].set_xlabel('Utilization Factor')\n    axes[i].set_ylabel('Frequency')\n\n# Adjusting layout for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe histograms above depict the distribution of utilization factors for each nuclear plant across different years. Here are some observations:\n\nThe distribution of utilization factors is fairly symmetric in most years, with a slight skewness in some years.\nThe majority of the nuclear plants seem to operate with a high utilization factor, mostly around 90 to 100.\nThere are a few years (e.g., 2016, 2020) where the distribution appears to be more spread out, indicating higher variability in the utilization factors.\n\n\n\n\n\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n\n# Flattening the 2D array of axes for easy iteration\naxes = axes.flatten()\n\n# Plotting the box plots\nfor i, year in enumerate(range(2013, 2023)):\n    sns.boxplot(x=nuclear_data[str(year)], ax=axes[i])\n    axes[i].set_title(f'Box Plot in {year}')\n    axes[i].set_xlabel('Utilization Factor')\n\n# Adjusting layout for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe box plots above provide a visual representation of the distribution of utilization factors for each year, highlighting the spread of the data and any potential outliers.\n\nThe median (represented by the line inside the box) is consistently high across all years, mostly above 90.\nThe interquartile range (IQR, represented by the height of the box) is relatively narrow, indicating that the middle 50% of the data points are close to each other.\nThere are some potential outliers in various years, particularly in 2016 and 2020, where the utilization factors are significantly lower than the rest of the data."
  },
  {
    "objectID": "Exploration.html#correlation-analysis",
    "href": "Exploration.html#correlation-analysis",
    "title": "Exploration",
    "section": "",
    "text": "# Calculating the correlation matrix\ncorrelation_matrix = nuclear_data.iloc[:, 1:].corr()\n\n# Creating a heatmap to visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\nplt.title('Correlation Matrix of Utilization Factors (2013-2022)')\nplt.show()\n\n\n\n\n\nMost of the correlations between different years are positive, suggesting that a higher utilization factor in one year is generally associated with a higher utilization factor in another year.\nThe correlation coefficients are mostly in the range of 0.2 to 0.8, indicating a moderate positive relationship between the years.\nThere are some years with higher correlations to each other (e.g., 2017 and 2019 with a correlation of 0.84), suggesting a stronger relationship in performance between these years."
  },
  {
    "objectID": "Exploration.html#hypothesis-generation",
    "href": "Exploration.html#hypothesis-generation",
    "title": "Exploration",
    "section": "",
    "text": "Based on the observed patterns and relationships in the data, we might formulate hypotheses such as:\n\nH1: The utilization factor of a nuclear plant in one year is positively related to its utilization factor in the subsequent year.\nH2: There are certain nuclear plants that consistently perform better or worse than others across all years.\nH3: The variability in utilization factors has decreased or increased over the years."
  },
  {
    "objectID": "Exploration.html#identifying-outliers",
    "href": "Exploration.html#identifying-outliers",
    "title": "Exploration",
    "section": "",
    "text": "# Calculating the average utilization factor for each nuclear plant over the years\nnuclear_data['Average'] = nuclear_data.iloc[:, 1:].mean(axis=1)\n\n# Displaying the nuclear plants sorted by their average utilization factor\nsorted_plants = nuclear_data[['Plant', 'Average']].sort_values(by='Average', ascending=False)\nprint(sorted_plants)\n\n                                 Plant  Average\n32                        Peach Bottom    99.24\n6   Calvert Cliffs Nuclear Power Plant    98.61\n14          Dresden Generating Station    97.56\n23                            Limerick    97.48\n4             Byron Generating Station    97.41\n36      Quad Cities Generating Station    97.28\n2         Braidwood Generation Station    97.23\n28                              Oconee    96.19\n46                              Vogtle    95.82\n22          LaSalle Generating Station    95.60\n24                             McGuire    95.06\n34           Point Beach Nuclear Plant    94.76\n7                              Catawba    94.67\n26     Nine Mile Point Nuclear Station    94.46\n35                      Prairie Island    94.27\n37       R E Ginna Nuclear Power Plant    94.23\n43                               Surry    94.19\n39                            Seabrook    94.09\n1                        Beaver Valley    93.91\n29  PSEG Hope Creek Generating Station    93.65\n27                          North Anna    93.62\n8                Clinton Power Station    93.27\n19                              Harris    93.11\n15                       Edwin I Hatch    92.77\n41                 South Texas Project    92.73\n10                       Comanche Peak    92.57\n31                          Palo Verde    92.48\n11                         Davis Besse    92.38\n21                     Joseph M Farley    92.21\n33                               Perry    91.67\n44                        Turkey Point    91.17\n3                         Browns Ferry    91.13\n25                           Millstone    90.92\n18                        H B Robinson    90.59\n40                            Sequoyah    90.56\n13                       Donald C Cook    90.50\n42                            St Lucie    89.98\n20                 James A Fitzpatrick    89.85\n45                          V C Summer    89.73\n9          Columbia Generating Station    89.21\n12                       Diablo Canyon    89.13\n30       PSEG Salem Generating Station    88.34\n47                         Waterford 3    87.99\n38                          River Bend    87.07\n48             Watts Bar Nuclear Plant    85.61\n0                 Arkansas Nuclear One    85.13\n49       Wolf Creek Generating Station    84.91\n5                             Callaway    82.47\n16                               Fermi    79.67\n17                          Grand Gulf    73.88\n\n\n\n\nThe nuclear plants with the highest average utilization factors over the years are:\n\nPeach Bottom: 99.24\nCalvert Cliffs Nuclear Power Plant: 98.61\nDresden Generating Station: 97.56\nLimerick: 97.48\nByron Generating Station: 97.41\n\n\n\n\nThe nuclear plants with the lowest average utilization factors are:\n\nGrand Gulf: 73.88\nFermi: 79.67\nCallaway: 82.47\nWolf Creek Generating Station: 84.91\nArkansas Nuclear One: 85.13\n\n\n# Defining performance tiers based on the average utilization factor\nbins = [0, 80, 90, 95, 100]\nlabels = ['Low Performer (&lt;80)', 'Below Average (80-90)', 'Above Average (90-95)', 'High Performer (&gt;95)']\nnuclear_data['Performance Tier'] = pd.cut(nuclear_data['Average'], bins=bins, labels=labels, right=False)\n\n# Calculating the number of plants in each performance tier\nperformance_tiers = nuclear_data['Performance Tier'].value_counts().sort_index()\n\n# Displaying the number of plants in each performance tier\nprint(performance_tiers)\n\nPerformance Tier\nLow Performer (&lt;80)       2\nBelow Average (80-90)    12\nAbove Average (90-95)    25\nHigh Performer (&gt;95)     11\nName: count, dtype: int64\n\n\nThe nuclear plants have been categorized into different performance tiers based on their average utilization factor:\n\nLow Performer (&lt;80): 2 plants\nBelow Average (80-90): 12 plants\nAbove Average (90-95): 25 plants\nHigh Performer (&gt;95): 11 plants\n\n\n# Function to identify outliers using the IQR method\ndef identify_outliers(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = data[(data[column] &lt; (Q1 - 1.5 * IQR)) | (data[column] &gt; (Q3 + 1.5 * IQR))]\n    return outliers[['Plant', column]]\n\n# Identifying outliers for each year\noutliers_dict = {}\nfor year in range(2013, 2023):\n    outliers = identify_outliers(nuclear_data, str(year))\n    if not outliers.empty:\n        outliers_dict[year] = outliers\n\n# Displaying the outliers for each year\nprint(outliers_dict)\n\n{2013:                             Plant  2013\n0            Arkansas Nuclear One  73.5\n16                          Fermi  69.4\n49  Wolf Creek Generating Station  69.6, 2014:           Plant  2014\n11  Davis Besse  74.4, 2016:                             Plant  2016\n17                     Grand Gulf  47.9\n30  PSEG Salem Generating Station  76.8\n48        Watts Bar Nuclear Plant  62.6, 2017:                           Plant  2017\n9   Columbia Generating Station  78.8\n17                   Grand Gulf  60.0\n48      Watts Bar Nuclear Plant  69.0, 2018:          Plant  2018\n16       Fermi  74.2\n17  Grand Gulf  56.4, 2019:           Plant  2019\n38   River Bend  75.8\n47  Waterford 3  74.1, 2020:          Plant  2020\n5     Callaway  70.7\n16       Fermi  60.6\n17  Grand Gulf  52.6, 2021:          Plant  2021\n5     Callaway  39.3\n45  V C Summer  76.5, 2022:           Plant  2022\n16        Fermi  66.7\n17   Grand Gulf  70.1\n47  Waterford 3  77.0}\n\n\n\n\n\nOutliers have been identified for each year based on the IQR method. Here are the plants that were identified as outliers in each year:\n\n2013:\n\nArkansas Nuclear One (73.5)\nFermi (69.4)\nWolf Creek Generating Station (69.6)\n\n2014:\n\nDavis Besse (74.4)\n\n2016:\n\nGrand Gulf (47.9)\nPSEG Salem Generating Station (76.8)\nWatts Bar Nuclear Plant (62.6)\n\n2017:\n\nColumbia Generating Station (78.8)\nGrand Gulf (60.0)\nWatts Bar Nuclear Plant (69.0)\n\n2018:\n\nFermi (74.2)\nGrand Gulf (56.4)\n\n2019:\n\nRiver Bend (75.8)\nWaterford 3 (74.1)\n\n2020:\n\nCallaway (70.7)\nFermi (60.6)\nGrand Gulf (52.6)\n\n2021:\n\nCallaway (39.3)\nV C Summer (76.5)\n\n2022:\n\nFermi (66.7)\nGrand Gulf (70.1)\nWaterford 3 (77.0)"
  },
  {
    "objectID": "Exploration.html#summary",
    "href": "Exploration.html#summary",
    "title": "Exploration",
    "section": "",
    "text": "Through the exploratory data analysis of the nuclear energy utilization dataset, we gained valuable insights into the performance of various nuclear plants over a span of 10 years, from 2013 to 2022. Below is a summary of the key findings and insights:\n\n\n\nThe majority of the nuclear plants have high utilization factors, mostly ranging from 90 to 100.\nThe distribution of utilization factors is fairly symmetric in most years, though there is some variability and a few years show a wider spread of values.\n\n\n\n\n\nThe utilization factors across different years are generally positively correlated, suggesting that a plant’s performance in one year is related to its performance in other years.\n\n\n\n\n\nWe identified plants that consistently perform well, such as Peach Bottom, Calvert Cliffs Nuclear Power Plant, and Dresden Generating Station, with average utilization factors above 97.\nConversely, plants like Grand Gulf, Fermi, and Callaway were identified as low performers, with average utilization factors below 85.\n\n\n\n\n\nThe plants were categorized into performance tiers, with 11 plants identified as high performers, 25 as above average, 12 as below average, and 2 as low performers.\n\n\n\n\n\nOutliers were identified in each year, pointing to plants with significantly lower utilization factors compared to the rest.\nSome plants, such as Grand Gulf and Fermi, appeared as outliers in multiple years, indicating consistent issues that may require further investigation."
  },
  {
    "objectID": "Classification.html",
    "href": "Classification.html",
    "title": "Classification",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "Cleaning.html#cleaning-3",
    "href": "Cleaning.html#cleaning-3",
    "title": "Cleaning",
    "section": "Cleaning:",
    "text": "Cleaning:\n\n# https://www-sciencedirect-com.proxy.library.georgetown.edu/science/article/pii/S030142151830377X\n\nimport fitz\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Return the path to the txt file\npdf_reader = fitz.open('../Data/Pre-Clean/Energy_Policy.pdf')\n\n# Open the txt file to write the extracted text\nwith open('../Data/Cleaned/Energy_Policy.txt', \"w\") as txt_file:\n    # Iterate through each page in the PDF file\n    for page_num in range(pdf_reader.page_count):\n        # Get the current page\n        page = pdf_reader.load_page(page_num)\n        \n        # Extract text from the current page\n        text = page.get_text()\n        \n        # Write the text to the txt file\n        txt_file.write(text)\n\n# Close the PDF reader\npdf_reader.close()"
  },
  {
    "objectID": "Cleaning.html#result-3",
    "href": "Cleaning.html#result-3",
    "title": "Cleaning",
    "section": "Result:",
    "text": "Result:\nContents lists available at ScienceDirect Energy Policy journal homepage: www.elsevier.com/locate/enpol “I can live with nuclear energy if…”: Exploring public perceptions of nuclear energy in Singapore Shirley S. Hoa,⁎, Jiemin Looia, Agnes S.F. Chuaha, Alisius D. Leonga, Natalie Pangb a Wee Kim Wee School of Communication and Information, Nanyang Technological University, 31 Nanyang Link, Singapore 637718, Singapore b IPS Social Lab, Institute of Policy Studies, Lee Kuan Yew School of Public Policy, National University of Singapore, 20 Evans Road, Singapore 259365, Singapore A R T I C L E I N F O Keywords: Nuclear energy Southeast Asia Singapore Psychometric paradigm Source credibility theory A B S T R A C T Considering the growing salience of nuclear energy in Southeast Asia, this study examines public perceptions of nuclear energy in Singapore, a technologically-advanced and aﬄuent nation well-equipped to develop nuclear energy capabilities. Drawing from the source credibility theory, this study examines the public’s credibility perceptions of nuclear-related information sources, and their trust in potential stakeholders. Guided by the psychometric paradigm, this study also explores public perceptions of risks, beneﬁts, and support. Four focus group discussions were conducted with Singaporeans aged 18–69. Participants across diﬀerent age groups (e.g., Millennials, Generation X, Baby Boomers) concurred in their trust of potential stakeholders, risk perception, cost perception, and support. Intergenerational diﬀerences were observed for participants’ media use, credibility perceptions of nuclear-related information sources, and beneﬁt perception. This study contributed theoretically by applying the source credibility theory and psychometric paradigm in an under-studied context. Practical implications were provided for policymakers and communication practitioners to eﬀectively evaluate public awareness and acceptance for nuclear energy. Directions for future research were discussed. In conclusion, intergenerational similarities were observed for Singaporeans’ perceptions of risks, costs, and support. Meanwhile intergenerational diﬀerences were noted for their credibility perceptions of nuclear-related in- formation sources, trust in potential stakeholders, and beneﬁt perception. 1. Introduction Since its inception, nuclear energy development is an issue that has divided public perceptions (Ho, 2016). Supporters have advocated nuclear energy as a solution to several environmental issues due to its low carbon emissions (International Atomic Energy Agency, 2014). Additionally, it can address global rising energy demands and provide energy security by generating a reliable and high output of electricity (Ertör-Akyazı et al., 2012; IAEA, 2014). However, critics have opposed it due to the risks of nuclear accidents, improper radioactive waste management, weaponization of nuclear energy, and the substantial operating costs of nuclear power plants (IAEA, 2014). Such polarized attitudes toward nuclear energy are also reﬂected in national energy policies globally (Ho, 2016). For instance, Belgium, France, Germany, and Switzerland have decided to gradually discontinue their use of nuclear energy (Reuters, 2017, 2018; World Nuclear Association, 2016). Conversely, the United States and the United Kingdom plan to construct new nuclear power plants (CNN, 2017). These divided atti- tudes toward nuclear energy are similarly observed in Asia. While Indonesia and Thailand have plans to adopt nuclear energy (WNA, 2016), Vietnam recently abandoned its nuclear energy adoption plans, citing safety concerns and high costs as a deterrent (Nguyen and Ho, 2016). Most public opinion research on nuclear energy are premised in countries that currently possess industry-scale nuclear facilities (Besley and McComas, 2015; Ho et al., 2018; Keller et al., 2012; Mah et al., 2014; Park and Ohm, 2014; Stoutenborough et al., 2013; Venables et al., 2012). However, few studies are conducted in countries which are in the preliminary phases of nuclear energy development (Ho et al., 2018). A recent meta-analysis on public perceptions of nuclear energy found that most studies were conducted in North America and Europe, while limited studies were conducted in SEA (Ho et al., 2018). As such, this study seeks to address the research gap by exploring public per- ceptions in SEA countries, particularly, Singapore. Additionally, this"
  },
  {
    "objectID": "Naïve_Bayes.html#prepare-for-naïve-bayes",
    "href": "Naïve_Bayes.html#prepare-for-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Prepare for Naïve Bayes",
    "text": "Prepare for Naïve Bayes\n\n# handling the outliers\n# Set any utilization factor in colum 2013-2022 above 100 to 100 in df\nimport numpy as np\nimport pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n\ndf = pd.read_csv('../data/Cleaned/EDA/Nuclear_Energy_Utilization_Factor.csv')\nprint(df.describe())\ndf[df.columns[1:]] = df[df.columns[1:]].apply(lambda x: np.where(x &gt; 100, 100, x))\nprint(df.describe())\n\n             2013        2014        2015        2016        2017        2018  \\\ncount   50.000000   50.000000   50.000000   50.000000   50.000000   50.000000   \nmean    90.836000   91.314000   91.376000   91.856000   91.498000   92.078000   \nstd      7.809381    6.001123    6.188091    9.806769    8.357314    7.834078   \nmin     69.400000   74.400000   74.500000   47.900000   60.000000   56.400000   \n25%     87.725000   88.650000   86.925000   90.200000   89.650000   90.125000   \n50%     92.550000   92.150000   93.100000   94.600000   93.800000   92.900000   \n75%     96.775000   95.750000   96.125000   97.700000   96.600000   97.400000   \nmax    101.300000  101.000000  100.800000  106.500000  103.500000  102.200000   \n\n             2019        2020        2021        2022  \ncount   50.000000   50.000000   50.000000   50.000000  \nmean    92.916000   92.062000   91.818000   92.318000  \nstd      6.336366    9.486788    9.272643    7.336815  \nmin     74.100000   52.600000   39.300000   66.700000  \n25%     89.375000   90.100000   89.850000   89.600000  \n50%     94.050000   95.000000   93.350000   94.400000  \n75%     98.075000   97.625000   96.550000   97.550000  \nmax    100.400000  102.900000  100.200000  100.400000  \n             2013        2014        2015        2016        2017        2018  \\\ncount   50.000000   50.000000   50.000000   50.000000   50.000000   50.000000   \nmean    90.796000   91.294000   91.342000   91.696000   91.354000   92.000000   \nstd      7.758523    5.969768    6.137509    9.621807    8.185433    7.744438   \nmin     69.400000   74.400000   74.500000   47.900000   60.000000   56.400000   \n25%     87.725000   88.650000   86.925000   90.200000   89.650000   90.125000   \n50%     92.550000   92.150000   93.100000   94.600000   93.800000   92.900000   \n75%     96.775000   95.750000   96.125000   97.700000   96.600000   97.400000   \nmax    100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n\n             2019        2020        2021        2022  \ncount   50.000000   50.000000   50.000000   50.000000  \nmean    92.900000   91.962000   91.814000   92.296000  \nstd      6.317646    9.389136    9.268996    7.312666  \nmin     74.100000   52.600000   39.300000   66.700000  \n25%     89.375000   90.100000   89.850000   89.600000  \n50%     94.050000   95.000000   93.350000   94.400000  \n75%     98.075000   97.625000   96.550000   97.550000  \nmax    100.000000  100.000000  100.000000  100.000000  \n\n\nThis part is just like data cleaning, the goal is to make sure that the utilization fill into the range from 0 to 100%. But in the real world there are cases where utilization goes over 100%, meaning that there could be more places that need more electrocites that the power plane can generate. For this case just going to make anything that is over 100% to 100%."
  },
  {
    "objectID": "Naïve_Bayes.html#feature-selection",
    "href": "Naïve_Bayes.html#feature-selection",
    "title": "Naïve Bayes",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n# Calculate the mean utilization factor from 2013 to 2021\ndf['Mean_Utilization_2013_2021'] = df.loc[:, '2013':'2021'].mean(axis=1)\n\n# Convert the utilization factor for 2022 into categories\ndf['Utilization_2022_Category'] = df['2022'].apply(lambda x: 'High' if x &gt;= 90 else 'Low')\n\n# Prepare the data\nX = df[['Mean_Utilization_2013_2021']]\ny = df['Utilization_2022_Category']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nThis part prepares data for the Naive Bayes Classifier, as all the columns are concerning each other as they are just utilization reports of each year. Our goal for the Native Bayes Classifier is to find out the utilization rate of future years from that date of the year before."
  },
  {
    "objectID": "Naïve_Bayes.html#naïve-bayes",
    "href": "Naïve_Bayes.html#naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes",
    "text": "Naïve Bayes\n\n# Initialize the Gaussian Naive Bayes classifier\ngnb = GaussianNB()\n\n# Train the classifier\ngnb.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = gnb.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n        High       0.86      0.86      0.86         7\n         Low       0.67      0.67      0.67         3\n\n    accuracy                           0.80        10\n   macro avg       0.76      0.76      0.76        10\nweighted avg       0.80      0.80      0.80        10\n\n\n\n\n\n\n\nAccuracy and General Performance\n\nThe model achieved an accuracy of 80%, which indicates a relatively high level of accuracy in predictions. However, this is based on a small test set of 10 instances, which might not be sufficient to generalize the model’s performance to unseen data.\n\n\n\nPrecision and Recall\n\nThe model showed better precision and recall for the ‘High’ category compared to the ‘Low’ category. This indicates that the model is more reliable when predicting high utilization factors.\n\nHigh Utilization Category: Both precision and recall are 0.86, showing a balanced performance.\nLow Utilization Category: Both precision and recall are 0.67, which is lower compared to the ‘High’ category. This could be a point of improvement for the model.\n\n\n\n\nF1-Score\n\nThe F1-Score is also higher for the ‘High’ category (0.86) compared to the ‘Low’ category (0.67). Since F1-Score is the harmonic mean of precision and recall, this again highlights the model’s stronger performance in predicting high utilization factors.\n\n\n\nConfusion Matrix\n\nThe confusion matrix reveals that the model made very few mistakes, with only one false positive and one false negative. However, due to the small size of the test set, each mistake has a significant impact on the performance metrics.\n\n\n\nConclusion\n\nThe Gaussian Naive Bayes classifier provides a good starting point for predicting nuclear energy utilization categories based on historical data. The model demonstrates decent accuracy, but there is potential for enhancement, particularly in improving precision and recall for the “Low” category. The exploratory data analysis and visualizations offer valuable insights into the distribution and trends of utilization factors across different power plants. Future work could explore more sophisticated models, feature engineering, or additional data sources to further refine the predictions and gain deeper insights.​"
  },
  {
    "objectID": "Naïve_Bayes.html#prepare-for-naïve-bayes-1",
    "href": "Naïve_Bayes.html#prepare-for-naïve-bayes-1",
    "title": "Naïve Bayes",
    "section": "Prepare for Naïve Bayes",
    "text": "Prepare for Naïve Bayes\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nwith open('../data/Cleaned/Energy_Policy.txt', 'r', encoding='utf-8') as file:\n    text_data = file.read()\n\nThis part is just input the text data that are been extracted from pdf in the cleaning tab."
  },
  {
    "objectID": "Naïve_Bayes.html#feature-selection-1",
    "href": "Naïve_Bayes.html#feature-selection-1",
    "title": "Naïve Bayes",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n# Define keywords related to nuclear energy\nnuclear_keywords = [\"nuclear\", \"radiation\", \"reactor\", \"uranium\", \"plutonium\"]\n\n# Segment the text into sentences\nsentences = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s', text_data)\n\n# Function to label sentences based on the presence of nuclear-related keywords\ndef label_sentences(sentences, keywords):\n    labels = []\n    for sentence in sentences:\n        if any(keyword.lower() in sentence.lower() for keyword in keywords):\n            labels.append(1)  # Positive class (related to nuclear energy)\n        else:\n            labels.append(0)  # Negative class (not related to nuclear energy)\n    return labels\n\n# Label the sentences\nlabels = label_sentences(sentences, nuclear_keywords)\n\n\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Initialize stop words and lemmatizer\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Function to preprocess text\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = text.split()\n    # Lowercase and remove stop words\n    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n    return ' '.join(tokens)\n\n# Preprocess each sentence\npreprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n\n# Splitting the data into training, validation, and testing sets (80-10-10 split)\nX_train, X_temp, y_train, y_temp = train_test_split(preprocessed_sentences, labels, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Initialize a TF-IDF Vectorizer\nvectorizer = TfidfVectorizer()\n\n# Transform the text data into TF-IDF vectors\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_val_tfidf = vectorizer.transform(X_val)\nX_test_tfidf = vectorizer.transform(X_test)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/jackyzhang/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/jackyzhang/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\nStep 1: Labeling the Data\n\nDefine Keywords: We’ll use the keywords “nuclear”, “radiation”, “reactor”, “uranium”, and “plutonium” to identify text related to nuclear energy.\nSegment the Text: Divide the text into sentences.\nLabel the Segments: Label each sentence based on whether it contains any of the defined keywords. Sentences with keywords will be labeled as 1 (positive class), and sentences without keywords will be labeled as 0 (negative class).\n\n\n\nStep 2: Preprocessing the Data\nNext, we will pre process the data. This involves the following steps:\n\nCleaning the Text: Removing any special characters, numbers, or unnecessary white space.\nTokenization: Splitting the text into individual words.\nLowercasing: Converting all text to lowercase to ensure uniformity.\nRemoving Stop Words: Removing common words that do not contribute much meaning to the text.\nStemming or Lemmatization: Reducing words to their base or root form."
  },
  {
    "objectID": "Naïve_Bayes.html#naïve-bayes-1",
    "href": "Naïve_Bayes.html#naïve-bayes-1",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes",
    "text": "Naïve Bayes\n\n# Initialize a Multinomial Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the Naive Bayes classifier\nnb_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions on the validation set\ny_val_pred = nb_classifier.predict(X_val_tfidf)\n\n# Evaluate the model on the validation set\nclassification_rep = classification_report(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\n\n# Print the classification report and confusion matrix\nprint('Classification Report:\\n', classification_rep)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      1.00      0.93        60\n           1       1.00      0.67      0.80        27\n\n    accuracy                           0.90        87\n   macro avg       0.93      0.83      0.87        87\nweighted avg       0.91      0.90      0.89        87\n\n\n\n\n\n\n\nAccuracy and General Performance\n\nThe model achieved an accuracy of 90%, which indicates a relatively high level of accuracy in predictions. However, this is based on a small test, which might not be sufficient to generalize the model’s performance to unseen data.\n\n\n\nClassification Report on Validation Data\n\nPrecision: For the negative class (0), it’s 0.87, and for the positive class (1), it’s 1.00.\nRecall: For the negative class, it’s 1.00, and for the positive class, it’s 0.67.\nF1-Score: For the negative class, it’s 0.93, and for the positive class, it’s 0.80.\nAccuracy: The overall accuracy of the model on the validation set is 90%.\n\n\n\nConfusion Matrix\n\nThe confusion matrix reveals that the model made very few mistakes, with only zero false positive and seven false negative. However, due to the small size of the test set, each mistake has a significant impact on the performance metrics.\n\n\n\nConclusion\n\nThe slight drop in accuracy from training to validation indicates that the model might be overfitting to the training data. However, the drop is not very large, suggesting that the overfitting is mild.\nThe model is performing well on both the training and validation sets, showing its ability to generalize to unseen data, although there’s room for improvement, especially in capturing more of the positive class (related to nuclear energy)."
  },
  {
    "objectID": "Cleaning.html#code",
    "href": "Cleaning.html#code",
    "title": "Cleaning",
    "section": "Code",
    "text": "Code\n\n# https://www.epa.gov/radnet/radnet-csv-file-downloads#DC\nimport pandas as pd\nimport glob\n\n# Read in the data\nfiles_path = sorted(glob.glob('../Data/Pre-Clean/rad_net/*.csv'))\ndfs = []\nfor file in files_path:\n    df = pd.read_csv(file)\n    dfs.append(df)\n\nfull_df = pd.concat(dfs, ignore_index=True)\n\n# Drop the GAMMA COUNT RATE R02 (CPM),GAMMA COUNT RATE R03 (CPM),GAMMA COUNT RATE R04 (CPM),GAMMA COUNT RATE R05 (CPM),GAMMA COUNT RATE R06 (CPM),GAMMA COUNT RATE R07 (CPM),GAMMA COUNT RATE R08 (CPM),GAMMA COUNT RATE R09 (CPM),STATUS cloums\nfull_df = full_df.drop(['SAMPLE COLLECTION TIME','GAMMA COUNT RATE R02 (CPM)','GAMMA COUNT RATE R03 (CPM)','GAMMA COUNT RATE R04 (CPM)','GAMMA COUNT RATE R05 (CPM)','GAMMA COUNT RATE R06 (CPM)','GAMMA COUNT RATE R07 (CPM)','GAMMA COUNT RATE R08 (CPM)','GAMMA COUNT RATE R09 (CPM)','STATUS'], axis=1)\n\n# Drop the rows with missing values\nfull_df = full_df.dropna()\nprint(full_df)\n\n        LOCATION_NAME  DOSE EQUIVALENT RATE (nSv/h)\n0       AK: ANCHORAGE                          33.0\n1       AK: ANCHORAGE                          32.0\n2       AK: ANCHORAGE                          31.0\n3       AK: ANCHORAGE                          32.0\n4       AK: ANCHORAGE                          31.0\n...               ...                           ...\n875462     WY: CASPER                          98.0\n875463     WY: CASPER                         101.0\n875464     WY: CASPER                         103.0\n875465     WY: CASPER                          98.0\n875466     WY: CASPER                         102.0\n\n[577599 rows x 2 columns]"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "For this project we are going to use clustering on DOSE EQUIVALENT RATE (nSv/h) and try to find out that if we are able to find out if we are able to find out and group different range of rate."
  },
  {
    "objectID": "Reduction.html",
    "href": "Reduction.html",
    "title": "Reduction",
    "section": "",
    "text": "This data set is about radiation reading in different state around the US, and the columns are LOCATION_NAME, SAMPLE COLLECTION TIME,DOSE EQUIVALENT RATE (nSv/h),GAMMA COUNT RATE R02 (CPM),GAMMA COUNT RATE R03 (CPM),GAMMA COUNT RATE R04 (CPM),GAMMA COUNT RATE R05 (CPM),GAMMA COUNT RATE R06 (CPM),GAMMA COUNT RATE R07 (CPM),GAMMA COUNT RATE R08 (CPM),GAMMA COUNT RATE R09 (CPM),STATUS.\nMost of the columns are not useful so I just kelp LOCATION_NAME and DOSE EQUIVALENT RATE (nSv/h).\n\nimport torch \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndata = pd.read_csv('../data/Cleaned/merged_data.csv')\n#data = data = data.sample(n=1000)\n# One-hot encoding the 'LOCATION_NAME' column\nencoder = OneHotEncoder(sparse=False)\nlocation_encoded = encoder.fit_transform(data[['LOCATION_NAME']])\n\n# Create a DataFrame from the encoded columns\nlocation_encoded_df = pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['LOCATION_NAME']))\n\n# Standard scaling\nscaler = StandardScaler()\ndose_scaled = scaler.fit_transform(data[['DOSE EQUIVALENT RATE (nSv/h)']])\n\n# Create a DataFrame from the scaled column\ndose_scaled_df = pd.DataFrame(dose_scaled, columns=['DOSE EQUIVALENT RATE (nSv/h)'])\npreprocessed_data = pd.concat([location_encoded_df, dose_scaled_df], axis=1)\npreprocessed_data.head()\n\n# Applying PCA\npca = PCA()\npca.fit(preprocessed_data)\n\n# Plotting the Cumulative Summation of the Explained Variance\nplt.figure(figsize=(8, 6))\nplt.plot(pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\nplt.title('Cumulative Explained Variance by PCA Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\npca_2 = PCA(n_components=2)\npca_2 = pca_2.fit_transform(preprocessed_data)\n\n# Creating a DataFrame for the PCA results\npca_df = pd.DataFrame(data=pca_2, columns=['PC1', 'PC2'])\n\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_df['PC1'], pca_df['PC2'])\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nThe first plot shows a curve that flattens out as the number of components increases, which indicates that each additional component accounts for a smaller increment in the explained variance. The ‘elbow’ of the plot is the point where the explained variance starts to increase at a slower rate, and it is often used to decide on the number of components to use for further analysis. In this plot, the elbow appears somewhat subtle but seems to occur around 20-30 components.\nThe second plot is a scatter plot of the first two principal components derived from PCA. This plot is used to visualize the data in the reduced-dimensional space created by PCA. Points that are close together are similar with respect to the principal components, and clusters may represent groups with similar properties."
  },
  {
    "objectID": "Reduction.html#pca",
    "href": "Reduction.html#pca",
    "title": "Reduction",
    "section": "PCA:",
    "text": "PCA:\n\nLinear Relationships: PCA is effective at preserving linear relationships between variables. It projects data onto orthogonal axes (principal components) that maximize variance.\nGlobal Structure Preservation: It maintains the global structure of the data, making it suitable for capturing the overall distribution and relative distances between high-variance points.\nInformation Loss: Some information, especially about low-variance features, may be lost in the dimensionality reduction process."
  },
  {
    "objectID": "Reduction.html#t-sne",
    "href": "Reduction.html#t-sne",
    "title": "Reduction",
    "section": "t-SNE:",
    "text": "t-SNE:\n\nNon-linear Relationships: t-SNE excels at preserving local structures and non-linear relationships. It focuses on maintaining the similarity between nearby points, often revealing clusters or groups in the data.\nGlobal vs. Local Structure: While excellent at revealing local patterns, t-SNE might not accurately represent the global structure. Distances between clusters can be misleading.\nDensity Preservation: It tends to preserve the density of the data, which can highlight substructures within clusters."
  },
  {
    "objectID": "Reduction.html#conclusion",
    "href": "Reduction.html#conclusion",
    "title": "Reduction",
    "section": "Conclusion",
    "text": "Conclusion\n\nPCA is suitable for large-scale exploratory analysis and when linear relationships are sufficient. Its interpretability and efficiency make it a standard choice for preliminary data analysis.\nt-SNE is more effective for detailed, local structure discovery in smaller datasets, particularly for revealing hidden clusters and patterns that PCA might miss."
  },
  {
    "objectID": "Clustering.html#k-means-clustering",
    "href": "Clustering.html#k-means-clustering",
    "title": "Clustering",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Menas clustering start by randomly k point and then interactively moves them to shorten the distance until getting closer to the point. Morover, when choosing the model, the Elbow Methold is used to determined the suitable number of clusters. The Silhouette Method assesses how similar an object is to its own cluster compared to others."
  },
  {
    "objectID": "Clustering.html#dbscan",
    "href": "Clustering.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nThis types of cluster point out what point are closer tougher, and make outlier lie alone in low density area. DBSCAN would form clusters in dense areas and identify isolated trees in sparse areas as outliers. Moreover, DBSCAN uses epsilon to find the neighborhood."
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering",
    "href": "Clustering.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nThis model builds a hierarchical of cluster by using divisive or agglomerate approach. Like building a decision tree with the point on the leave. Dendrograms are used to visualize the formation of clusters in hierarchical clustering."
  },
  {
    "objectID": "Clustering.html#elbow",
    "href": "Clustering.html#elbow",
    "title": "Clustering",
    "section": "Elbow",
    "text": "Elbow\n\nwcss = []  # Within-cluster sum of squares\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(data_cluster)\n    wcss.append(kmeans.inertia_)\n\n# Plotting the results of the Elbow method\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 11), wcss, marker='o')\nplt.title('Elbow Method For Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()"
  },
  {
    "objectID": "Clustering.html#silhousette-score",
    "href": "Clustering.html#silhousette-score",
    "title": "Clustering",
    "section": "Silhousette Score",
    "text": "Silhousette Score\n\nX = data[[\"DOSE EQUIVALENT RATE (nSv/h)\"]]\n\n# Finding the optimal number of clusters using silhouette scores\nsilhouette_scores = []\nk_range = range(2, 11)  # Testing for number of clusters from 2 to 10\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    cluster_labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plotting the silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, silhouette_scores, marker='o')\nplt.title(\"Silhouette Scores for Different Numbers of Clusters\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.xticks(k_range)\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Clustering.html#data-selection",
    "href": "Clustering.html#data-selection",
    "title": "Clustering",
    "section": "Data selection",
    "text": "Data selection\n\nfrom sklearn.cluster import KMeans, Birch, AgglomerativeClustering, DBSCAN\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndata = pd.read_csv('../data/Cleaned/merged_data.csv')\ndata = data.sample(n=10000)\n# take only the columns that are needed for clustering\ndata_cluster = data[['DOSE EQUIVALENT RATE (nSv/h)']]"
  },
  {
    "objectID": "Clustering.html#hyper-parameter-tuning",
    "href": "Clustering.html#hyper-parameter-tuning",
    "title": "Clustering",
    "section": "Hyper-parameter tuning",
    "text": "Hyper-parameter tuning\n\nElbow\n\nwcss = []  # Within-cluster sum of squares\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(data_cluster)\n    wcss.append(kmeans.inertia_)\n\n# Plotting the results of the Elbow method\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 11), wcss, marker='o')\nplt.title('Elbow Method For Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nSilhousette Score\n\nX = data[[\"DOSE EQUIVALENT RATE (nSv/h)\"]]\n\n# Finding the optimal number of clusters using silhouette scores\nsilhouette_scores = []\nk_range = range(2, 11)  # Testing for number of clusters from 2 to 10\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    cluster_labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plotting the silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, silhouette_scores, marker='o')\nplt.title(\"Silhouette Scores for Different Numbers of Clusters\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.xticks(k_range)\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Clustering.html#dbscan-clustering",
    "href": "Clustering.html#dbscan-clustering",
    "title": "Clustering",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\n\neps_values = np.arange(0.1, 2.0, 0.1)\nmin_samples = 5  # Default value for min_samples\n\n# Extracting the 'DOSE EQUIVALENT RATE (nSv/h)' column for clustering\ndose_data = data['DOSE EQUIVALENT RATE (nSv/h)'].values.reshape(-1, 1)\n\n# Dictionary to store the number of clusters for each eps value\nnum_clusters = {}\n\n# Perform DBSCAN clustering for each eps value and store the number of clusters\nfor eps in eps_values:\n    db = DBSCAN(eps=eps, min_samples=min_samples).fit(dose_data)\n    labels = db.labels_\n    \n    # Number of clusters in labels, ignoring noise if present\n    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n    num_clusters[eps] = n_clusters\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(list(num_clusters.keys()), list(num_clusters.values()), marker='o')\nplt.title('DBSCAN Clustering: Number of Clusters for Different eps Values')\nplt.xlabel('eps Value')\nplt.ylabel('Number of Clusters')\nplt.grid(True)\nplt.show()\n\n\n\n\n\neps_selected = 1.0\ndb_selected = DBSCAN(eps=eps_selected, min_samples=min_samples).fit(dose_data)\nlabels_selected = db_selected.labels_\n\n# Number of clusters in labels, ignoring noise if present\nn_clusters_selected = len(set(labels_selected)) - (1 if -1 in labels_selected else 0)\n\n# Adding the cluster labels to the original dataset\ndata['Cluster'] = labels_selected\n\n# Displaying the number of clusters and the first few rows of the dataset with cluster labels\nn_clusters_selected, data.head()\n\n(2,\n            LOCATION_NAME  DOSE EQUIVALENT RATE (nSv/h)  Cluster\n 471595       TX: HOUSTON                          36.0        0\n 132294  FL: JACKSONVILLE                          52.0        0\n 188384     IL: CHAMPAIGN                          70.0        0\n 486675    TX: SAN ANGELO                          47.0        0\n 358506       NY: YAPHANK                          40.0        0)"
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering-1",
    "href": "Clustering.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\ndose_data = data[['DOSE EQUIVALENT RATE (nSv/h)']].values\n\n# Using agglomerative hierarchical clustering\nlinked = linkage(dose_data, method='ward')\n\n# Plotting the dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked, labels=data['LOCATION_NAME'].values, orientation='top')\nplt.title('Hierarchical Clustering Dendrogram (Agglomerative)')\nplt.xlabel('Location')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "Clustering.html#k-means",
    "href": "Clustering.html#k-means",
    "title": "Clustering",
    "section": "K-Means",
    "text": "K-Means\n\nElbow\n\nwcss = []  # Within-cluster sum of squares\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(data_cluster)\n    wcss.append(kmeans.inertia_)\n\n# Plotting the results of the Elbow method\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 11), wcss, marker='o')\nplt.title('Elbow Method For Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nSilhousette Score\n\nX = data[[\"DOSE EQUIVALENT RATE (nSv/h)\"]]\n\n# Finding the optimal number of clusters using silhouette scores\nsilhouette_scores = []\nk_range = range(2, 11)  # Testing for number of clusters from 2 to 10\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    cluster_labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plotting the silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, silhouette_scores, marker='o')\nplt.title(\"Silhouette Scores for Different Numbers of Clusters\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.xticks(k_range)\nplt.grid(True)\nplt.show()\n\n\n\n\n\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data[['DOSE EQUIVALENT RATE (nSv/h)']])\n\n# Performing k-means clustering with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=0)\nclusters = kmeans.fit_predict(data_scaled)\n\n# Adding the cluster information to the original dataframe\ndata['Cluster'] = clusters\n\n# Display the first few rows with cluster labels\nclustered_data_head = data.head()\n\n# Summary of clusters\ncluster_summary = data['Cluster'].value_counts()\n\nclustered_data_head, cluster_summary\n\n(           LOCATION_NAME  DOSE EQUIVALENT RATE (nSv/h)  Cluster\n 471595       TX: HOUSTON                          36.0        0\n 132294  FL: JACKSONVILLE                          52.0        1\n 188384     IL: CHAMPAIGN                          70.0        1\n 486675    TX: SAN ANGELO                          47.0        0\n 358506       NY: YAPHANK                          40.0        0,\n Cluster\n 1    4306\n 0    3857\n 2    1837\n Name: count, dtype: int64)"
  },
  {
    "objectID": "Decision_tree.html",
    "href": "Decision_tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Random Forest is like a council of wise advisors, each bringing their unique perspective to a problem. By leveraging the wisdom of the crowd, it arrives at decisions that are often more accurate and reliable than those made by a single decision tree. This blend of simplicity, robustness, and versatility makes Random Forest a valuable tool in our data-driven world.\n\n\n\nThey break down complex decisions into simpler, more manageable steps, making them a valuable tool in various applications, from business strategy to data analysis."
  },
  {
    "objectID": "Decision_tree.html#random-forest",
    "href": "Decision_tree.html#random-forest",
    "title": "Decision Tree",
    "section": "",
    "text": "Random Forest is like a council of wise advisors, each bringing their unique perspective to a problem. By leveraging the wisdom of the crowd, it arrives at decisions that are often more accurate and reliable than those made by a single decision tree. This blend of simplicity, robustness, and versatility makes Random Forest a valuable tool in our data-driven world."
  },
  {
    "objectID": "Decision_tree.html#decision-tree",
    "href": "Decision_tree.html#decision-tree",
    "title": "Decision Tree",
    "section": "",
    "text": "They break down complex decisions into simpler, more manageable steps, making them a valuable tool in various applications, from business strategy to data analysis."
  },
  {
    "objectID": "Decision_tree.html#prepareing-data",
    "href": "Decision_tree.html#prepareing-data",
    "title": "Decision Tree",
    "section": "Prepareing data",
    "text": "Prepareing data\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_csv('../data/Cleaned/merged_data.csv')\n# Analyzing the distribution of the \"DOSE EQUIVALENT RATE (nSv/h)\"\nplt.figure(figsize=(10,6))\nplt.hist(data[\"DOSE EQUIVALENT RATE (nSv/h)\"], bins=30, color='blue', alpha=0.7)\nplt.title('Distribution of DOSE EQUIVALENT RATE (nSv/h)')\nplt.xlabel('DOSE EQUIVALENT RATE (nSv/h)')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nlow_threshold, high_threshold = data[\"DOSE EQUIVALENT RATE (nSv/h)\"].quantile([0.33, 0.67])\n\n# Applying the categorization\ndata['Category'] = pd.cut(data[\"DOSE EQUIVALENT RATE (nSv/h)\"], bins=[data[\"DOSE EQUIVALENT RATE (nSv/h)\"].min(), low_threshold, high_threshold, data[\"DOSE EQUIVALENT RATE (nSv/h)\"].max()], labels=[\"Low\", \"Medium\", \"High\"])\ndata.head()\n\n\n\n\n\n\n\n\nLOCATION_NAME\nDOSE EQUIVALENT RATE (nSv/h)\nCategory\n\n\n\n\n0\nAK: ANCHORAGE\n33.0\nLow\n\n\n1\nAK: ANCHORAGE\n32.0\nLow\n\n\n2\nAK: ANCHORAGE\n31.0\nLow\n\n\n3\nAK: ANCHORAGE\n32.0\nLow\n\n\n4\nAK: ANCHORAGE\n31.0\nLow\n\n\n\n\n\n\n\n\nclass_distribution = data['Category'].value_counts(normalize=True)\n\nprint(class_distribution)\n\n# Visualize the distribution\nclass_distribution.plot(kind='bar')\nplt.title('Class Distribution')\nplt.show()\n\nCategory\nLow       0.338109\nMedium    0.333086\nHigh      0.328806\nName: proportion, dtype: float64\n\n\n\n\n\n\nencoder = LabelEncoder()\ndata['LOCATION_NAME_ENCODED'] = encoder.fit_transform(data['LOCATION_NAME'])\ndata['Category_ENCODED'] = encoder.fit_transform(data['Category'])\n# Preparing the data\nX = data[['DOSE EQUIVALENT RATE (nSv/h)']]  # Feature\ny = data['Category_ENCODED']  # Target\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "Decision_tree.html#decision-tree-1",
    "href": "Decision_tree.html#decision-tree-1",
    "title": "Decision Tree",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\n# Creating the Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Training the classifier\nclf.fit(X_train, y_train)\n\n# Making predictions\ny_pred = clf.predict(X_test)\n\n# Evaluating the classifier\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(classification_rep)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     38107\n           1       1.00      1.00      1.00     39276\n           2       1.00      1.00      1.00     38122\n           3       1.00      1.00      1.00        15\n\n    accuracy                           1.00    115520\n   macro avg       1.00      1.00      1.00    115520\nweighted avg       1.00      1.00      1.00    115520"
  },
  {
    "objectID": "Decision_tree.html#random-forest-1",
    "href": "Decision_tree.html#random-forest-1",
    "title": "Decision Tree",
    "section": "Random Forest",
    "text": "Random Forest\n\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Training the model\nrf_classifier.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = rf_classifier.predict(X_test)\n\n# Generating a classification report\nclassification_report_result = classification_report(y_test, y_pred)\nprint(classification_report_result)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     38107\n           1       1.00      1.00      1.00     39276\n           2       1.00      1.00      1.00     38122\n           3       1.00      1.00      1.00        15\n\n    accuracy                           1.00    115520\n   macro avg       1.00      1.00      1.00    115520\nweighted avg       1.00      1.00      1.00    115520"
  },
  {
    "objectID": "Decision_tree.html#baseline-model",
    "href": "Decision_tree.html#baseline-model",
    "title": "Decision Tree",
    "section": "Baseline Model",
    "text": "Baseline Model\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=42)\ndummy_clf.fit(X_train, y_train)\n\n# Predicting the categories\ny_pred = dummy_clf.predict(X_test)\n\n# Evaluating the classifier\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00     38107\n           1       0.34      1.00      0.51     39276\n           2       0.00      0.00      0.00     38122\n           3       0.00      0.00      0.00        15\n\n    accuracy                           0.34    115520\n   macro avg       0.08      0.25      0.13    115520\nweighted avg       0.12      0.34      0.17    115520\n\n\n\nWe can see that Baseline Mode is not very accurate at predicting the Category."
  },
  {
    "objectID": "Decision_tree.html#model-turning",
    "href": "Decision_tree.html#model-turning",
    "title": "Decision Tree",
    "section": "Model Turning",
    "text": "Model Turning\n\nRandom Forest\n\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Training the model\nrf_classifier.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = rf_classifier.predict(X_test)\n\n# Generating a classification report\nclassification_report_result = classification_report(y_test, y_pred)\nprint(classification_report_result)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     38107\n           1       1.00      1.00      1.00     39276\n           2       1.00      1.00      1.00     38122\n           3       1.00      1.00      1.00        15\n\n    accuracy                           1.00    115520\n   macro avg       1.00      1.00      1.00    115520\nweighted avg       1.00      1.00      1.00    115520\n\n\n\nFrom the result we can see that Random Forest almost has the prefect accuracy\n\n\nDecision Tree\n\n# Removing the entry with missing category\ndata_cleaned = data.dropna(subset=['Category'])\n\n# Encoding the 'Category' column\nlabel_encoder = LabelEncoder()\ndata_cleaned['Category_encoded'] = label_encoder.fit_transform(data_cleaned['Category'])\n\n# Splitting the data into features and target variable\nX = data_cleaned[['DOSE EQUIVALENT RATE (nSv/h)']]  # Features\ny = data_cleaned['Category_encoded']               # Target variable\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Creating the Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Training the model\nclf.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = clf.predict(X_test)\n\n# Evaluating the model\nevaluation_report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\nprint(evaluation_report)\n\n              precision    recall  f1-score   support\n\n        High       1.00      1.00      1.00     57042\n         Low       1.00      1.00      1.00     58511\n      Medium       1.00      1.00      1.00     57716\n\n    accuracy                           1.00    173269\n   macro avg       1.00      1.00      1.00    173269\nweighted avg       1.00      1.00      1.00    173269\n\n\n\nFrom the result we can see that Decision Tree has the prefect accuracy"
  },
  {
    "objectID": "Decision_tree.html#result",
    "href": "Decision_tree.html#result",
    "title": "Decision Tree",
    "section": "Result",
    "text": "Result\n\nDecision Tree\n\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plotting the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',\n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n\n\n\n\n# Plotting the decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(clf, \n          filled=True, \n          feature_names=['DOSE EQUIVALENT RATE'], \n          class_names=label_encoder.classes_,\n          rounded=True, \n          proportion=False)\nplt.title(\"Decision Tree Classifier\")\nplt.show()"
  }
]